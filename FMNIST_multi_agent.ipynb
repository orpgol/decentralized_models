{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from scipy.stats import entropy, ks_2samp\n",
    "from scipy.special import kl_div\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard\n",
    "\n",
    "import traceback\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logname = 'logs/decentralized_multi_agent'\n",
    "logging.basicConfig(filename=logname,\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%H:%M:%S',\n",
    "                            level=logging.DEBUG)\n",
    "\n",
    "logging.info(\"Running Decentralized Learning test\")\n",
    "\n",
    "logger = logging.getLogger('Decentralized_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd39dff1410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproduce randomness for fair comparison\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Standard transformations for improving CIFAR10. \n",
    "\n",
    "# Transformations A\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Transformations B\n",
    "RC   = transforms.RandomCrop(32, padding=4)\n",
    "RHF  = transforms.RandomHorizontalFlip()\n",
    "RVF  = transforms.RandomVerticalFlip()\n",
    "NRM  = transforms.Normalize([0.5], [0.5])\n",
    "TT   = transforms.ToTensor()\n",
    "TPIL = transforms.ToPILImage()\n",
    "\n",
    "# Transforms object for trainset with augmentation\n",
    "transform_with_aug = transforms.Compose([TPIL, RC, RHF, TT, NRM])\n",
    "# Transforms object for testset with NO augmentation\n",
    "transform_no_aug   = transforms.Compose([TPIL, TT, NRM])\n",
    "\n",
    "# Downloading/Louding CIFAR10 data\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../data/cifar10', train=True,\n",
    "                                        download=True, transform=transform_with_aug)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='../data/cifar10', train=False,\n",
    "                                       download=True, transform=transform_no_aug)\n",
    "\n",
    "\n",
    "classDict = {'T-shirt/top':0, 'Trouser':1, 'Pullover':2, 'Dress':3, 'Coat':4, 'Sandal':5, 'Shirt':6, 'Sneaker':7, 'Bag':8, 'Ankle boot':9}\n",
    "\n",
    "# Separating trainset/testset data/label\n",
    "x_train  = trainset.data\n",
    "x_test   = testset.data\n",
    "y_train  = trainset.targets\n",
    "y_test   = testset.targets\n",
    "\n",
    "# Define a function to separate CIFAR classes by class index\n",
    "\n",
    "def get_class_i(x, y, i):\n",
    "    \"\"\"\n",
    "    x: trainset.train_data or testset.test_data\n",
    "    y: trainset.train_labels or testset.test_labels\n",
    "    i: class label, a number between 0 to 9\n",
    "    return: x_i\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array\n",
    "    y = np.array(y)\n",
    "    # Locate position of labels that equal to i\n",
    "    pos_i = np.argwhere(y == i)\n",
    "    # Convert the result into a 1-D list\n",
    "    pos_i = list(pos_i[:,0])\n",
    "    # Collect all data that match the desired label\n",
    "    x_i = [x[j] for j in pos_i]\n",
    "    \n",
    "    return x_i\n",
    "\n",
    "class DatasetMaker(Dataset):\n",
    "    def __init__(self, datasets, transformFunc = transform_no_aug):\n",
    "        \"\"\"\n",
    "        datasets: a list of get_class_i outputs, i.e. a list of list of images for selected classes\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.lengths  = [len(d) for d in self.datasets]\n",
    "        self.transformFunc = transformFunc\n",
    "    def __getitem__(self, i):\n",
    "        class_label, index_wrt_class = self.index_of_which_bin(self.lengths, i)\n",
    "        img = self.datasets[class_label][index_wrt_class]\n",
    "        img = self.transformFunc(img)\n",
    "        return img, class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lengths)\n",
    "    \n",
    "    def index_of_which_bin(self, bin_sizes, absolute_index, verbose=False):\n",
    "        \"\"\"\n",
    "        Given the absolute index, returns which bin it falls in and which element of that bin it corresponds to.\n",
    "        \"\"\"\n",
    "        # Which class/bin does i fall into?\n",
    "        accum = np.add.accumulate(bin_sizes)\n",
    "        if verbose:\n",
    "            print(\"accum =\", accum)\n",
    "        bin_index  = len(np.argwhere(accum <= absolute_index))\n",
    "        if verbose:\n",
    "            print(\"class_label =\", bin_index)\n",
    "        # Which element of the fallent class/bin does i correspond to?\n",
    "        index_wrt_class = absolute_index - np.insert(accum, 0, 0)[bin_index]\n",
    "        if verbose:\n",
    "            print(\"index_wrt_class =\", index_wrt_class)\n",
    "\n",
    "        return bin_index, index_wrt_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are saving a fraction of random data to be used in training\n",
    "frac = int(len(x_train) * 0.05)\n",
    "x_reserve = x_train[:frac]\n",
    "y_reserve = y_train[:frac]\n",
    "x_train = x_train[frac:]\n",
    "y_train = y_train[frac:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Usage ================== #\n",
    "\n",
    "# \n",
    "trainset1 = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train, y_train, classDict['T-shirt/top']), \n",
    "         get_class_i(x_train, y_train, classDict['Trouser']), \n",
    "         get_class_i(x_train, y_train, classDict['Pullover']),\n",
    "        [],[],[],[],[],[],[]],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset2 = \\\n",
    "    DatasetMaker(\n",
    "        [[],[],[],\n",
    "         get_class_i(x_train, y_train, classDict['Dress']), \n",
    "         get_class_i(x_train, y_train, classDict['Coat']), \n",
    "         get_class_i(x_train, y_train, classDict['Sandal']), \n",
    "         [],[],[],[]],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset3 = \\\n",
    "    DatasetMaker(\n",
    "        [[],[],[],[],[],[],\n",
    "         get_class_i(x_train, y_train, classDict['Shirt']), \n",
    "         get_class_i(x_train, y_train, classDict['Sneaker']), \n",
    "         get_class_i(x_train, y_train, classDict['Bag']), \n",
    "         get_class_i(x_train, y_train, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset4 = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train, y_train, classDict['T-shirt/top']), \n",
    "         get_class_i(x_train, y_train, classDict['Trouser']), \n",
    "         get_class_i(x_train, y_train, classDict['Pullover']), \n",
    "         get_class_i(x_train, y_train, classDict['Dress']), \n",
    "         get_class_i(x_train, y_train, classDict['Coat']),\n",
    "         get_class_i(x_train, y_train, classDict['Sandal']), \n",
    "         get_class_i(x_train, y_train, classDict['Shirt']), \n",
    "         get_class_i(x_train, y_train, classDict['Sneaker']), \n",
    "         get_class_i(x_train, y_train, classDict['Bag']), \n",
    "         get_class_i(x_train, y_train, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "reserved = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_reserve, y_reserve, classDict['T-shirt/top']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Trouser']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Pullover']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Dress']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Coat']),\n",
    "         get_class_i(x_reserve, y_reserve, classDict['Sandal']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Shirt']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Sneaker']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Bag']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "# reservedB = \\\n",
    "#     DatasetMaker(\n",
    "#         [get_class_i(x_reserveB, y_reserveB, classDict['plane']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['car']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['bird']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['cat']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['deer']),\n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['dog']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['frog']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['horse']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['ship']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['truck'])],\n",
    "#         transform_with_aug\n",
    "#     )\n",
    "testset  = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_test, y_test, classDict['T-shirt/top']), \n",
    "         get_class_i(x_test, y_test, classDict['Trouser']), \n",
    "         get_class_i(x_test, y_test, classDict['Pullover']), \n",
    "         get_class_i(x_test, y_test, classDict['Dress']), \n",
    "         get_class_i(x_test, y_test, classDict['Coat']),\n",
    "         get_class_i(x_test, y_test, classDict['Sandal']), \n",
    "         get_class_i(x_test, y_test, classDict['Shirt']), \n",
    "         get_class_i(x_test, y_test, classDict['Sneaker']), \n",
    "         get_class_i(x_test, y_test, classDict['Bag']), \n",
    "         get_class_i(x_test, y_test, classDict['Ankle boot'])],\n",
    "        transform_no_aug\n",
    "    )\n",
    "\n",
    "superset = torch.utils.data.ConcatDataset([trainset3,reserved])\n",
    "supersetB = torch.utils.data.ConcatDataset([trainset2,reserved])\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': False}\n",
    "\n",
    "# Create datasetLoaders from trainset and testset\n",
    "trainsetLoader1   = DataLoader(trainset1, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader2   = DataLoader(trainset2, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader3   = DataLoader(trainset3, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader4   = DataLoader(trainset4, batch_size=128, shuffle=True , **kwargs)\n",
    "reservedLoader    = DataLoader(superset, batch_size=128, shuffle=True , **kwargs)\n",
    "reservedLoaderB   = DataLoader(supersetB, batch_size=128, shuffle=True , **kwargs)\n",
    "testsetLoader     = DataLoader(testset , batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.3    # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test that things are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(reservedLoader))\n",
    "imshow(torchvision.utils.make_grid(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1, 32, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide on a GPU to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "\n",
    "# device2 = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decentralized Pairwise Knowledge Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models used in knowledge transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, (torch.nn.Linear)):\n",
    "        torch.nn.init.sparse_(m.weight, sparsity=0.33)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (torch.nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class Decenter(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter, self).__init__()\n",
    "        if len(shape) == 1:\n",
    "            shape = shape[0]\n",
    "            self.dim = 0\n",
    "        elif len(shape) == 2:\n",
    "            shape = shape[1]\n",
    "            self.dim = 1\n",
    "        self.translation = torch.nn.Sequential(\n",
    "#             torch.nn.Tanh(),\n",
    "            torch.nn.Linear(shape*3, shape)\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source1, source2, target):\n",
    "        x = torch.cat((source1, source2, target), self.dim)\n",
    "#         x = torch.cat((torch.flatten(source), torch.flatten(target)), 0)\n",
    "#         x = torch.add(torch.flatten(source).to(\"cpu\"), torch.flatten(target).to(\"cpu\"))\n",
    "        res = self.translation(x)\n",
    "#         res = res.reshape(target.shape)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class Interpolate(torch.nn.Module):\n",
    "    def __init__(self, size, mode):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = torch.nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n",
    "        return x\n",
    "    \n",
    "class Reshape(torch.nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Decenter_pooled(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter_pooled, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.translation = torch.nn.Sequential(\n",
    "#             torch.nn.BatchNorm2d(channels_out),\n",
    "#             torch.nn.AdaptiveAvgPool2d(1),\n",
    "            Interpolate(size=1, mode='bilinear'),\n",
    "            Reshape(shape[0], shape[1]*2),\n",
    "            torch.nn.Linear(shape[1]*2, shape[1]*shape[-1]*shape[-1]),\n",
    "#             Reshape(shape[0], shape[1] ,1 ,1),\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        x = torch.cat((source, target), 1)\n",
    "        res = self.translation(x)\n",
    "        res = res.view(self.shape[0], self.shape[1], self.shape[2], self.shape[3])\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class Decenter_conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter_conv, self).__init__()\n",
    "        self.shape = shape\n",
    "        channels_in = shape[1]*3\n",
    "        channels_out = shape[1]\n",
    "        self.translation = torch.nn.Sequential(\n",
    "#             torch.nn.BatchNorm2d(channels_in),\n",
    "            torch.nn.Conv2d(channels_in, channels_out, 1, stride=1, padding=0)\n",
    "#             torch.nn.ConvTranspose2d(channels_in, channels_out, 3, stride=1, padding=1)\n",
    "#             torch.nn.Linear(shape[0]*2, shape[0]*4),\n",
    "#             torch.nn.Dropout(p=0.5),\n",
    "#             torch.nn.Linear(shape[0]*2, shape[0]),\n",
    "#             torch.nn.AdaptiveAvgPool2d((shape[-2],shape[-1])),\n",
    "#             torch.nn.Conv2d(channels_out, channels_out, 3, stride=1, padding=1)\n",
    "\n",
    "        )\n",
    "        self.translation2 = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveMaxPool2d((shape[-2],shape[-1])),\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source1, source2, target):\n",
    "        x = torch.cat((source1, source2, target), 1)\n",
    "#         x = x.reshape(-1, x.shape[0])\n",
    "#         x = torch.cat((torch.flatten(source), torch.flatten(target)), 0)\n",
    "#         x = torch.add(torch.flatten(source).to(\"cpu\"), torch.flatten(target).to(\"cpu\"))\n",
    "        res = self.translation(x)\n",
    "#         res = res.reshape(self.shape)\n",
    "        return res\n",
    "\n",
    "class Special(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Special, self).__init__()\n",
    "        self.w1 = torch.nn.Parameter(torch.tensor(0.3333333))\n",
    "        self.w2 = torch.nn.Parameter(torch.tensor(0.3333333))\n",
    "        self.w3 = torch.nn.Parameter(torch.tensor(0.3333333))\n",
    "        \n",
    "    def forward(self, source1, source2, target, x):\n",
    "        s1 = copy.deepcopy(source1.state_dict())\n",
    "        s2 = copy.deepcopy(source2.state_dict())\n",
    "        t = copy.deepcopy(target.state_dict())\n",
    "        \n",
    "        w_glob = copy.deepcopy(t)\n",
    "        for k in mdlzAC.keys():\n",
    "            w_glob[k] = s1[k]*self.w1+s2[k]*self.w2+t[k]*self.w3\n",
    "#         target.load_state_dict(w_glob)\n",
    "        \n",
    "        return w_glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the local training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "### Hooks are used to investigate the effect of gradiant decent on weights. Not necessary for simply reproducing paper results. \n",
    "grad_dict: dict = {}\n",
    "def fc_hook(layer_name, grad_input, grad_output): \n",
    "    if layer_name not in grad_dict:\n",
    "        grad_dict[layer_name] = {}\n",
    "        grad_dict[layer_name][\"grad_input\"] = []\n",
    "        grad_dict[layer_name][\"grad_output\"] = []\n",
    "        grad_dict[layer_name][\"labels\"] = []\n",
    "        \n",
    "#     print(grad_input)\n",
    "#     print(grad_output)\n",
    "    grad_dict[layer_name][\"grad_input\"].append(grad_input[0].cpu().numpy())\n",
    "    grad_dict[layer_name][\"grad_output\"].append(grad_output[0].cpu().numpy())\n",
    "    \n",
    "# def reserve_step(source, target):\n",
    "    \n",
    "\n",
    "matlst = []\n",
    "fclst = []\n",
    "reslist = []\n",
    "\n",
    "## Option are used for simulating remote agents using pairwise knowledge transfer \n",
    "options = {0: ['trainA', 'validA','reservedAB'], \n",
    "           1: ['trainB','validB','reservedBA'],\n",
    "           2: ['trainC','validC','reservedCA','validC']}\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "def train_model(dataloders, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "#     writer = SummaryWriter('runs/') \n",
    "\n",
    "    since = time.time()\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    best_model_wts = 0.0\n",
    "    best_acc = 0.0\n",
    "    dataset_sizes = {'trainA': len(dataloders['trainA'].sampler),\n",
    "                     'trainB': len(dataloders['trainB'].sampler),\n",
    "                     'trainC': len(dataloders['trainC'].sampler),\n",
    "                     'reservedA': len(dataloders['reservedA'].sampler),\n",
    "                     'reservedB': len(dataloders['reservedB'].sampler),\n",
    "                     'reservedCA': len(dataloders['reservedCA'].sampler),\n",
    "                     'reservedAB': len(dataloders['reservedAB'].sampler),\n",
    "                     'reservedBA': len(dataloders['reservedBA'].sampler),\n",
    "                     'validA': len(dataloders['validA'].sampler),\n",
    "                     'validB': len(dataloders['validB'].sampler),\n",
    "                     'validC': len(dataloders['validC'].sampler)}\n",
    "\n",
    "    i = z = 0\n",
    "    ivc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['trainA', 'validA','trainB','validB','trainC','validC','reservedCA','validC']:  ## this sequense simualtes a half mesh configuation\n",
    "#         choice = np.random.choice(range(3), replace=False)\n",
    "#         for phase in options[choice]:\n",
    "            if phase not in ['validA','validB','validC']:\n",
    "                model[phase].train(True)\n",
    "            else:\n",
    "                model['trainA'].train(False)\n",
    "                model['trainB'].train(False)\n",
    "                model['trainC'].train(False)\n",
    "            \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0        \n",
    "                    \n",
    "            for inputs, labels in dataloders[phase]:\n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                optimizer[phase].zero_grad()\n",
    "                \n",
    "                ## if its a transfer phase, swap layers using the pipline models\n",
    "#                 if phase in ['reservedCA','reservedAB', 'reservedBA']:\n",
    "                if phase == 'reservedCA':\n",
    "                        sd = model[phase].state_dict()\n",
    "                        for key, value in mdlzAC.items():\n",
    "                            shape = model['trainC'].state_dict()[key].shape\n",
    "                            mdl = value[1](shape).to(device)\n",
    "                            checkpoint = torch.load(value[0])\n",
    "                            mdl.load_state_dict(checkpoint['model_state_dict'])\n",
    "                            mdl.eval()\n",
    "                            sd[key] = mdl(copy.deepcopy(model['trainA'].state_dict()[key]),\n",
    "                                          copy.deepcopy(model['trainB'].state_dict()[key]),\n",
    "                                          copy.deepcopy(model['trainC'].state_dict()[key]))\n",
    "    #                         torch.save({'model_state_dict': mdl.state_dict()}, value[0])\n",
    "\n",
    "    #                         if key == 'conv1.weight':\n",
    "    #                             matlst.append(sd[key])\n",
    "    #                         elif key == 'fc.weight':\n",
    "    #                             fclst.append(sd[key])\n",
    "                        model[phase].load_state_dict(sd)\n",
    "        \n",
    "                #print(list(model[phase].parameters()))\n",
    "                outputs = model[phase](inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                ## if its a transfer phase, compute remote loss term\n",
    "                if phase in ['reservedCA','reservedAB','reservedBA']:\n",
    "                    loss_a = criterion['trainC'](outputs, labels)\n",
    "#                     batch_size = labels.shape[0]\n",
    "#                     # Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
    "#                     y = labels.reshape(-1,1)\n",
    "#                     # One hot encoding buffer that you create out of the loop and just keep reusing\n",
    "#                     y_onehot = torch.FloatTensor(batch_size, 10).to(device)\n",
    "\n",
    "#                     # In your for loop\n",
    "#                     y_onehot.zero_()\n",
    "#                     y_onehot.scatter_(1, y, 1)\n",
    "\n",
    "                    if phase == 'reservedCA':\n",
    "                        outputs2 = model['trainA'](inputs)\n",
    "                        outputs3 = model['trainB'](inputs)\n",
    "                    elif phase == 'reservedAB':\n",
    "                        outputs2 = model['trainC'](inputs)\n",
    "                        outputs3 = model['trainB'](inputs)\n",
    "                    elif phase == 'reservedBA':\n",
    "                        outputs2 = model['trainC'](inputs)\n",
    "                        outputs3 = model['trainA'](inputs)\n",
    "    \n",
    "#                     sm = torch.nn.Softmax(dim=1)\n",
    "                    sm = torch.nn.LogSoftmax(dim=1)\n",
    "                    outputs = sm(outputs)\n",
    "                    outputs2 = sm(outputs2)\n",
    "                    outputs3 = sm(outputs3)\n",
    "                    loss_b = criterion[phase](outputs, outputs2)\n",
    "                    loss_c = criterion[phase](outputs, outputs3)\n",
    "#                     loss = (loss_a + (loss_b + loss_c)*0.5)*0.5\n",
    "\n",
    "                    loss_d = criterion[phase](sd['fc.weight'], model['trainA'].state_dict()['fc.weight'])\n",
    "                    loss_e = criterion[phase](sd['fc.weight'], model['trainB'].state_dict()['fc.weight'])\n",
    "                    loss = (loss_a*0.7 + (loss_b + loss_c)*0.3 + (loss_d + loss_e)*0.0)\n",
    "                    \n",
    "                ## if its not a transfer phase, compute local loss term\n",
    "                else:\n",
    "                    loss = criterion[phase](outputs, labels)\n",
    "\n",
    "                ## if its not a validation phase, optimize our models\n",
    "                if phase not in ['validA','validB','validC']:\n",
    "                    loss.backward()\n",
    "                    if phase == 'reservedCA':\n",
    "#                       loss_b.backward(retain_graph=False)\n",
    "                        for key, value in mdlzAC.items():\n",
    "                            shape = model['trainC'].state_dict()[key].shape\n",
    "                            mdl = value[1](shape).to(device)\n",
    "                            opti = torch.optim.AdamW(mdl.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "                            checkpoint = torch.load(value[0])\n",
    "                            mdl.load_state_dict(checkpoint['model_state_dict'])\n",
    "                            mdl.train(True)\n",
    "                            opti.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#                             opti.zero_grad()\n",
    "                            opti.step()\n",
    "                            torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                                        'optimizer_state_dict': opti.state_dict(),\n",
    "                                        'factor': checkpoint['factor'],\n",
    "                                        'patience': checkpoint['patience'],\n",
    "                                        'scheduler_state_dict': checkpoint['scheduler_state_dict']},\n",
    "                                        value[0])\n",
    "                    else:\n",
    "                        optimizer[phase].step()\n",
    "                            \n",
    "                    \n",
    "                    ## Back prop hook\n",
    "#                     grad_dict[\"fc\"][\"labels\"].append(labels.cpu().numpy())\n",
    "\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            ## update scheduler for each model\n",
    "            if phase in ['validA','validB','validC']:\n",
    "                scheduler[phase].step(running_loss)\n",
    "                \n",
    "            elif phase == 'reservedCA':\n",
    "                for key, value in mdlzAC.items():\n",
    "                    checkpoint = torch.load(value[0])\n",
    "                    sched = lr_scheduler.ReduceLROnPlateau(optimizerC, 'min', factor=checkpoint['factor'], patience=checkpoint['patience'])\n",
    "                    sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                    sched.step(running_loss)\n",
    "                    torch.save({'model_state_dict': checkpoint['model_state_dict'],\n",
    "                                'optimizer_state_dict': checkpoint['optimizer_state_dict'],\n",
    "                                'factor': checkpoint['factor'],\n",
    "                                'patience': checkpoint['patience'],\n",
    "                                'scheduler_state_dict': sched.state_dict()},\n",
    "                                value[0])\n",
    "\n",
    "#             elif phase == 'reservedAB':\n",
    "#                 for key, value in mdlzAB.items():\n",
    "#                     checkpoint = torch.load(value[0])\n",
    "#                     sched = lr_scheduler.ReduceLROnPlateau(optimizerA, 'min', factor=checkpoint['factor'], patience=checkpoint['patience'])\n",
    "#                     sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#                     sched.step(running_loss)\n",
    "#                     torch.save({'model_state_dict': checkpoint['model_state_dict'],\n",
    "#                                 'optimizer_state_dict': checkpoint['optimizer_state_dict'],\n",
    "#                                 'factor': checkpoint['factor'],\n",
    "#                                 'patience': checkpoint['patience'],\n",
    "#                                 'scheduler_state_dict': sched.state_dict()},\n",
    "#                                 value[0])\n",
    "                    \n",
    "#             elif phase == 'reservedBA':\n",
    "#                 for key, value in mdlzBA.items():\n",
    "#                     checkpoint = torch.load(value[0])\n",
    "#                     sched = lr_scheduler.ReduceLROnPlateau(optimizerB, 'min', factor=checkpoint['factor'], patience=checkpoint['patience'])\n",
    "#                     sched.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "#                     sched.step(running_loss)\n",
    "#                     torch.save({'model_state_dict': checkpoint['model_state_dict'],\n",
    "#                                 'optimizer_state_dict': checkpoint['optimizer_state_dict'],\n",
    "#                                 'factor': checkpoint['factor'],\n",
    "#                                 'patience': checkpoint['patience'],\n",
    "#                                 'scheduler_state_dict': sched.state_dict()},\n",
    "#                                 value[0])\n",
    "\n",
    "                \n",
    "            ## report results\n",
    "            if phase not in ['validA','validB','validC']:\n",
    "                train_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                train_epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            else:\n",
    "                valid_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                valid_epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print('Epoch [{}/{}] phase: {} train loss: {:.4f} acc: {:.4f} ' \n",
    "                      'valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                        epoch, num_epochs - 1,\n",
    "                        phase,\n",
    "                        train_epoch_loss, train_epoch_acc, \n",
    "                        valid_epoch_loss, valid_epoch_acc))\n",
    "                print() \n",
    "                logger.info('Epoch [{}/{}] phase: {} train loss: {:.4f} acc: {:.4f} ' \n",
    "                      'valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                        epoch, num_epochs - 1,\n",
    "                        phase,\n",
    "                        train_epoch_loss, train_epoch_acc, \n",
    "                        valid_epoch_loss, valid_epoch_acc))\n",
    "                \n",
    "                ## Writing to tensorboard\n",
    "                if phase == 'validC':\n",
    "                    ivc += 1\n",
    "                    if ivc == 2:\n",
    "#                         writer.add_histogram('distribution centers/our_half_mesh_transfer', outputs, i)\n",
    "\n",
    "#                         writer.add_scalar('train/loss_our_half_mesh_tansfer', train_epoch_loss, epoch)\n",
    "#                         writer.add_scalar('train/accuracy_our_half_mesh_tansfer', train_epoch_acc, epoch)\n",
    "\n",
    "#                         writer.add_scalar('valid/loss_our_half_mesh_tansfer', valid_epoch_loss, epoch)\n",
    "#                         writer.add_scalar('valid/accuracy_our_half_mesh_tansfer', valid_epoch_acc, epoch)\n",
    "                        reslist.append(valid_epoch_acc.item())\n",
    "                        ivc = 0\n",
    "\n",
    "                \n",
    "            if phase in ['validC'] and valid_epoch_acc > best_acc:\n",
    "                best_acc = valid_epoch_acc\n",
    "                best_model_wts = model[phase].state_dict()\n",
    "\n",
    "            i+=1\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    logger.info('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     writer.close()\n",
    "    model['validC'].load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Define models for network of learners, optimizers, and loss terms\n",
    "\n",
    "resnetA = models.resnet50(pretrained=True)\n",
    "resnetB = models.resnet50(pretrained=True)\n",
    "resnetC = models.resnet50(pretrained=True)\n",
    "# freeze all model parameters\n",
    "# for param in resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# new final layer with 10 classes\n",
    "num_ftrsA = resnetA.fc.in_features\n",
    "resnetA.fc = torch.nn.Linear(num_ftrsA, 10)\n",
    "resnetA.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrsB = resnetB.fc.in_features\n",
    "resnetB.fc = torch.nn.Linear(num_ftrsB, 10)\n",
    "resnetB.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrsC = resnetC.fc.in_features\n",
    "resnetC.fc = torch.nn.Linear(num_ftrsC, 10)\n",
    "resnetC.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "def fc_backward_hook(module, grad_input, grad_output):  # module is Linear in this case. Ignored.\n",
    "        fc_hook(\"fc\", grad_input, grad_output)\n",
    "resnetA.fc_hook_handle = resnetA.fc.register_backward_hook(fc_backward_hook)\n",
    "resnetB.fc_hook_handle = resnetB.fc.register_backward_hook(fc_backward_hook)\n",
    "resnetC.fc_hook_handle = resnetC.fc.register_backward_hook(fc_backward_hook)\n",
    "\n",
    "\n",
    "def roc_auc_score_micro(y_pred_proba, y_true):\n",
    "    y_pred_proba = y_pred_proba.detach().cpu()\n",
    "    y_true = y_true.detach().cpu()\n",
    "    return metrics.roc_auc_score(\n",
    "        label_binarize(y_true, classes=list(range(y_pred_proba.shape[1]))).ravel(),\n",
    "        y_pred_proba.flatten())\n",
    "\n",
    "\n",
    "resnetA = resnetA.to(device)\n",
    "resnetB = resnetB.to(device)\n",
    "resnetC = resnetC.to(device)\n",
    "\n",
    "criterionA = torch.nn.CrossEntropyLoss()\n",
    "# criterionB = torch.nn.CrossEntropyLoss()\n",
    "# criterionA = torch.nn.KLDivLoss()\n",
    "criterionB = torch.nn.KLDivLoss(reduction = 'batchmean')\n",
    "# criterionB = torch.nn.KLDivLoss(reduction = 'mean')\n",
    "# criterionB = torch.nn.MSELoss()\n",
    "optimizerA = torch.optim.SGD(resnetA.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizerB = torch.optim.SGD(resnetB.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizerC = torch.optim.SGD(resnetC.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizerA = torch.optim.AdamW(resnetA.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "# optimizerB = torch.optim.AdamW(resnetB.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "# optimizerC = torch.optim.AdamW(resnetC.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# exp_lr_schedulerA = lr_scheduler.StepLR(optimizerA, step_size=5, gamma=0.01)\n",
    "# exp_lr_schedulerB = lr_scheduler.StepLR(optimizerB, step_size=5, gamma=0.01)\n",
    "# exp_lr_schedulerC = lr_scheduler.StepLR(optimizerC, step_size=5, gamma=0.2)\n",
    "exp_lr_schedulerA = lr_scheduler.ReduceLROnPlateau(optimizerA, 'min', factor=0.90, patience=500)\n",
    "exp_lr_schedulerB = lr_scheduler.ReduceLROnPlateau(optimizerB, 'min', factor=0.90, patience=500)\n",
    "exp_lr_schedulerC = lr_scheduler.ReduceLROnPlateau(optimizerC, 'min', factor=0.90, patience=500)\n",
    "\n",
    "\n",
    "def hwout(Hin, padding, dilation, kernel_size, stride):\n",
    "    return (Hin + 2 * padding - dilation * (kernel_size-1) - 1)/stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for finding largest layer\n",
    "\n",
    "max_layer = 0\n",
    "max_neurons = 0\n",
    "for prm in resnetC.named_parameters():\n",
    "    num_ftr = np.prod(prm[1].shape)\n",
    "    if num_ftr > max_neurons:\n",
    "         max_neurons = num_ftr\n",
    "         max_layer = prm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'bn1.weight',\n",
       " 'bn1.bias',\n",
       " 'layer1.0.conv1.weight',\n",
       " 'layer1.0.bn1.weight',\n",
       " 'layer1.0.bn1.bias',\n",
       " 'layer1.0.conv2.weight',\n",
       " 'layer1.0.bn2.weight',\n",
       " 'layer1.0.bn2.bias',\n",
       " 'layer1.0.conv3.weight',\n",
       " 'layer1.0.bn3.weight',\n",
       " 'layer1.0.bn3.bias',\n",
       " 'layer1.0.downsample.0.weight',\n",
       " 'layer1.0.downsample.1.weight',\n",
       " 'layer1.0.downsample.1.bias',\n",
       " 'layer1.1.conv1.weight',\n",
       " 'layer1.1.bn1.weight',\n",
       " 'layer1.1.bn1.bias',\n",
       " 'layer1.1.conv2.weight',\n",
       " 'layer1.1.bn2.weight',\n",
       " 'layer1.1.bn2.bias',\n",
       " 'layer1.1.conv3.weight',\n",
       " 'layer1.1.bn3.weight',\n",
       " 'layer1.1.bn3.bias',\n",
       " 'layer1.2.conv1.weight',\n",
       " 'layer1.2.bn1.weight',\n",
       " 'layer1.2.bn1.bias',\n",
       " 'layer1.2.conv2.weight',\n",
       " 'layer1.2.bn2.weight',\n",
       " 'layer1.2.bn2.bias',\n",
       " 'layer1.2.conv3.weight',\n",
       " 'layer1.2.bn3.weight',\n",
       " 'layer1.2.bn3.bias',\n",
       " 'layer2.0.conv1.weight',\n",
       " 'layer2.0.bn1.weight',\n",
       " 'layer2.0.bn1.bias',\n",
       " 'layer2.0.conv2.weight',\n",
       " 'layer2.0.bn2.weight',\n",
       " 'layer2.0.bn2.bias',\n",
       " 'layer2.0.conv3.weight',\n",
       " 'layer2.0.bn3.weight',\n",
       " 'layer2.0.bn3.bias',\n",
       " 'layer2.0.downsample.0.weight',\n",
       " 'layer2.0.downsample.1.weight',\n",
       " 'layer2.0.downsample.1.bias',\n",
       " 'layer2.1.conv1.weight',\n",
       " 'layer2.1.bn1.weight',\n",
       " 'layer2.1.bn1.bias',\n",
       " 'layer2.1.conv2.weight',\n",
       " 'layer2.1.bn2.weight',\n",
       " 'layer2.1.bn2.bias',\n",
       " 'layer2.1.conv3.weight',\n",
       " 'layer2.1.bn3.weight',\n",
       " 'layer2.1.bn3.bias',\n",
       " 'layer2.2.conv1.weight',\n",
       " 'layer2.2.bn1.weight',\n",
       " 'layer2.2.bn1.bias',\n",
       " 'layer2.2.conv2.weight',\n",
       " 'layer2.2.bn2.weight',\n",
       " 'layer2.2.bn2.bias',\n",
       " 'layer2.2.conv3.weight',\n",
       " 'layer2.2.bn3.weight',\n",
       " 'layer2.2.bn3.bias',\n",
       " 'layer2.3.conv1.weight',\n",
       " 'layer2.3.bn1.weight',\n",
       " 'layer2.3.bn1.bias',\n",
       " 'layer2.3.conv2.weight',\n",
       " 'layer2.3.bn2.weight',\n",
       " 'layer2.3.bn2.bias',\n",
       " 'layer2.3.conv3.weight',\n",
       " 'layer2.3.bn3.weight',\n",
       " 'layer2.3.bn3.bias',\n",
       " 'layer3.0.conv1.weight',\n",
       " 'layer3.0.bn1.weight',\n",
       " 'layer3.0.bn1.bias',\n",
       " 'layer3.0.conv2.weight',\n",
       " 'layer3.0.bn2.weight',\n",
       " 'layer3.0.bn2.bias',\n",
       " 'layer3.0.conv3.weight',\n",
       " 'layer3.0.bn3.weight',\n",
       " 'layer3.0.bn3.bias',\n",
       " 'layer3.0.downsample.0.weight',\n",
       " 'layer3.0.downsample.1.weight',\n",
       " 'layer3.0.downsample.1.bias',\n",
       " 'layer3.1.conv1.weight',\n",
       " 'layer3.1.bn1.weight',\n",
       " 'layer3.1.bn1.bias',\n",
       " 'layer3.1.conv2.weight',\n",
       " 'layer3.1.bn2.weight',\n",
       " 'layer3.1.bn2.bias',\n",
       " 'layer3.1.conv3.weight',\n",
       " 'layer3.1.bn3.weight',\n",
       " 'layer3.1.bn3.bias',\n",
       " 'layer3.2.conv1.weight',\n",
       " 'layer3.2.bn1.weight',\n",
       " 'layer3.2.bn1.bias',\n",
       " 'layer3.2.conv2.weight',\n",
       " 'layer3.2.bn2.weight',\n",
       " 'layer3.2.bn2.bias',\n",
       " 'layer3.2.conv3.weight',\n",
       " 'layer3.2.bn3.weight',\n",
       " 'layer3.2.bn3.bias',\n",
       " 'layer3.3.conv1.weight',\n",
       " 'layer3.3.bn1.weight',\n",
       " 'layer3.3.bn1.bias',\n",
       " 'layer3.3.conv2.weight',\n",
       " 'layer3.3.bn2.weight',\n",
       " 'layer3.3.bn2.bias',\n",
       " 'layer3.3.conv3.weight',\n",
       " 'layer3.3.bn3.weight',\n",
       " 'layer3.3.bn3.bias',\n",
       " 'layer3.4.conv1.weight',\n",
       " 'layer3.4.bn1.weight',\n",
       " 'layer3.4.bn1.bias',\n",
       " 'layer3.4.conv2.weight',\n",
       " 'layer3.4.bn2.weight',\n",
       " 'layer3.4.bn2.bias',\n",
       " 'layer3.4.conv3.weight',\n",
       " 'layer3.4.bn3.weight',\n",
       " 'layer3.4.bn3.bias',\n",
       " 'layer3.5.conv1.weight',\n",
       " 'layer3.5.bn1.weight',\n",
       " 'layer3.5.bn1.bias',\n",
       " 'layer3.5.conv2.weight',\n",
       " 'layer3.5.bn2.weight',\n",
       " 'layer3.5.bn2.bias',\n",
       " 'layer3.5.conv3.weight',\n",
       " 'layer3.5.bn3.weight',\n",
       " 'layer3.5.bn3.bias',\n",
       " 'layer4.0.conv1.weight',\n",
       " 'layer4.0.bn1.weight',\n",
       " 'layer4.0.bn1.bias',\n",
       " 'layer4.0.conv2.weight',\n",
       " 'layer4.0.bn2.weight',\n",
       " 'layer4.0.bn2.bias',\n",
       " 'layer4.0.conv3.weight',\n",
       " 'layer4.0.bn3.weight',\n",
       " 'layer4.0.bn3.bias',\n",
       " 'layer4.0.downsample.0.weight',\n",
       " 'layer4.0.downsample.1.weight',\n",
       " 'layer4.0.downsample.1.bias',\n",
       " 'layer4.1.conv1.weight',\n",
       " 'layer4.1.bn1.weight',\n",
       " 'layer4.1.bn1.bias',\n",
       " 'layer4.1.conv2.weight',\n",
       " 'layer4.1.bn2.weight',\n",
       " 'layer4.1.bn2.bias',\n",
       " 'layer4.1.conv3.weight',\n",
       " 'layer4.1.bn3.weight',\n",
       " 'layer4.1.bn3.bias',\n",
       " 'layer4.2.conv1.weight',\n",
       " 'layer4.2.bn1.weight',\n",
       " 'layer4.2.bn1.bias',\n",
       " 'layer4.2.conv2.weight',\n",
       " 'layer4.2.bn2.weight',\n",
       " 'layer4.2.bn2.bias',\n",
       " 'layer4.2.conv3.weight',\n",
       " 'layer4.2.bn3.weight',\n",
       " 'layer4.2.bn3.bias',\n",
       " 'fc.weight',\n",
       " 'fc.bias']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in resnetC.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "### Define our transfer pipeline and attach models, and parameters. Model chains are saved to disk for memory efficient execution.\n",
    "mdlzAC = dict()\n",
    "mdlzAB = dict()\n",
    "mdlzBA = dict()\n",
    "params = []\n",
    "model_dir = './multi_model_chain'\n",
    "conv_layers = []\n",
    "bn_layers = ['fc.weight']\n",
    "\n",
    "for prm in resnetC.named_parameters():\n",
    "# for prm in temp_list:\n",
    "#     if 'conv' in prm[0] or 'fc' in prm[0] or 'bn' in prm[0] or 'downsample' in prm[0]:\n",
    "    if prm[0] in bn_layers+conv_layers:\n",
    "#     if 'conv' in prm[0] or 'fc' in prm[0]:\n",
    "        try:\n",
    "#             mdl = Special().to(device)\n",
    "            if prm[1].dim() > 2:\n",
    "#                 if prm[0] not in conv_layers:\n",
    "#                     continue\n",
    "                mdl = Decenter_conv(prm[1].shape).to(device)\n",
    "                optimizer = torch.optim.AdamW(mdl.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "                exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=900)\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.1,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'A')\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.1,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'B')\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.1,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'C')\n",
    "#                 mdlzAC[prm[0]] = (mdl, optimizer, exp_lr_scheduler)\n",
    "                mdlzAC[prm[0]] = (model_dir + '/' + prm[0]+'A', Decenter_conv)\n",
    "#                 params += mdl.parameters()\n",
    "#                 pass\n",
    "            else:\n",
    "#                 if prm[0] not in bn_layers:\n",
    "#                     continue\n",
    "                mdl = Decenter(prm[1].shape).to(device)\n",
    "                optimizer = torch.optim.AdamW(mdl.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "                exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=900)\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.5,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'A')\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.5,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'B')\n",
    "                torch.save({'model_state_dict': mdl.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'factor': 0.5,\n",
    "                            'patience': 900,\n",
    "                            'scheduler_state_dict': exp_lr_scheduler.state_dict()},\n",
    "                           model_dir + '/' + prm[0]+'C')\n",
    "                mdlzAC[prm[0]] = (model_dir + '/' + prm[0]+'A', Decenter)\n",
    "#                 params += mdl.parameters()\n",
    "            \n",
    "            del mdl\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(\"Problem with: \" + prm[0] + \" Size: \" + str(num_ftr))\n",
    "            print(\"Error: \" + str(e))\n",
    "            traceback.print_exc()\n",
    "            print()\n",
    "            pass\n",
    "        \n",
    "# params += list(resnetC.parameters())\n",
    "\n",
    "# optimizerRB = torch.optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "# mdlAC = Special().to(device)\n",
    "# optimizerAC = torch.optim.AdamW(mdlAC.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "# exp_lr_schedulerAC = lr_scheduler.ReduceLROnPlateau(optimizerAC, 'min', factor=0.1, patience=900)\n",
    "# optimizerRC = torch.optim.AdamW(resnetC.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc.weight': ('./multi_model_chain/fc.weightA', __main__.Decenter)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlzAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define phases and associated pareameters\n",
    "dloaders = {'trainA':trainsetLoader1, 'trainB':trainsetLoader2, 'trainC':reservedLoader,\n",
    "            'validA':testsetLoader, 'validB':testsetLoader, 'validC':testsetLoader,\n",
    "            'reservedA':reservedLoader, 'reservedB':reservedLoader, 'reservedCA':reservedLoader, 'reservedAB':reservedLoader, 'reservedBA':reservedLoader}\n",
    "model = {'trainA':resnetA, 'trainB':resnetB, 'trainC':resnetC,\n",
    "         'validA':resnetA, 'validB':resnetB, 'validC':resnetC,\n",
    "         'reservedA':resnetA, 'reservedB':resnetB, 'reservedCA':resnetC, 'reservedAB':resnetA, 'reservedBA':resnetB}\n",
    "optimizer = {'trainA':optimizerA, 'trainB':optimizerB, 'trainC':optimizerC,\n",
    "             'validA':optimizerA, 'validB':optimizerB, 'validC':optimizerC,\n",
    "             'reservedA':optimizerA, 'reservedB':optimizerB, 'reservedCA':optimizerC, 'reservedAB':optimizerA, 'reservedBA':optimizerB}\n",
    "criterion = {'trainA':criterionA, 'trainB':criterionA, 'trainC':criterionA,\n",
    "             'validA':criterionA, 'validB':criterionA, 'validC':criterionA,\n",
    "             'reservedA':criterionB, 'reservedB':criterionB, 'reservedCA':criterionB, 'reservedAB':criterionB, 'reservedBA':criterionB}\n",
    "exp_lr_scheduler = {'trainA':exp_lr_schedulerA, 'trainB':exp_lr_schedulerB, 'trainC':exp_lr_schedulerC,\n",
    "             'validA':exp_lr_schedulerA, 'validB':exp_lr_schedulerB, 'validC':exp_lr_schedulerC,\n",
    "             'reservedA':exp_lr_schedulerA, 'reservedB':exp_lr_schedulerB, 'reservedCA':exp_lr_schedulerC, 'reservedAB':exp_lr_schedulerA, 'reservedBA':exp_lr_schedulerB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/59] phase: validA train loss: 0.0029 acc: 0.8714 valid loss: 0.0566 acc: 0.2798\n",
      "\n",
      "Epoch [0/59] phase: validB train loss: 0.0026 acc: 0.8835 valid loss: 0.0602 acc: 0.2757\n",
      "\n",
      "Epoch [0/59] phase: validC train loss: 0.0048 acc: 0.8199 valid loss: 0.0140 acc: 0.4419\n",
      "\n",
      "Epoch [1/59] phase: validA train loss: 0.0010 acc: 0.9570 valid loss: 0.0578 acc: 0.2823\n",
      "\n",
      "Epoch [1/59] phase: validB train loss: 0.0010 acc: 0.9529 valid loss: 0.0615 acc: 0.2811\n",
      "\n",
      "Epoch [1/59] phase: validC train loss: 0.0026 acc: 0.8983 valid loss: 0.0113 acc: 0.4953\n",
      "\n",
      "Epoch [2/59] phase: validA train loss: 0.0008 acc: 0.9683 valid loss: 0.0594 acc: 0.2844\n",
      "\n",
      "Epoch [2/59] phase: validB train loss: 0.0008 acc: 0.9597 valid loss: 0.0625 acc: 0.2815\n",
      "\n",
      "Epoch [2/59] phase: validC train loss: 0.0021 acc: 0.9124 valid loss: 0.0112 acc: 0.4955\n",
      "\n",
      "Epoch [3/59] phase: validA train loss: 0.0007 acc: 0.9690 valid loss: 0.0585 acc: 0.2860\n",
      "\n",
      "Epoch [3/59] phase: validB train loss: 0.0007 acc: 0.9651 valid loss: 0.0622 acc: 0.2836\n",
      "\n",
      "Epoch [3/59] phase: validC train loss: 0.0019 acc: 0.9189 valid loss: 0.0110 acc: 0.5026\n",
      "\n",
      "Epoch [4/59] phase: validA train loss: 0.0007 acc: 0.9706 valid loss: 0.0602 acc: 0.2876\n",
      "\n",
      "Epoch [4/59] phase: validB train loss: 0.0007 acc: 0.9646 valid loss: 0.0634 acc: 0.2837\n",
      "\n",
      "Epoch [4/59] phase: validC train loss: 0.0018 acc: 0.9226 valid loss: 0.0093 acc: 0.5558\n",
      "\n",
      "Epoch [5/59] phase: validA train loss: 0.0006 acc: 0.9723 valid loss: 0.0597 acc: 0.2898\n",
      "\n",
      "Epoch [5/59] phase: validB train loss: 0.0006 acc: 0.9692 valid loss: 0.0625 acc: 0.2853\n",
      "\n",
      "Epoch [5/59] phase: validC train loss: 0.0016 acc: 0.9284 valid loss: 0.0094 acc: 0.5622\n",
      "\n",
      "Epoch [6/59] phase: validA train loss: 0.0006 acc: 0.9747 valid loss: 0.0608 acc: 0.2893\n",
      "\n",
      "Epoch [6/59] phase: validB train loss: 0.0006 acc: 0.9687 valid loss: 0.0634 acc: 0.2874\n",
      "\n",
      "Epoch [6/59] phase: validC train loss: 0.0016 acc: 0.9338 valid loss: 0.0093 acc: 0.5842\n",
      "\n",
      "Epoch [7/59] phase: validA train loss: 0.0005 acc: 0.9756 valid loss: 0.0617 acc: 0.2888\n",
      "\n",
      "Epoch [7/59] phase: validB train loss: 0.0006 acc: 0.9703 valid loss: 0.0639 acc: 0.2874\n",
      "\n",
      "Epoch [7/59] phase: validC train loss: 0.0015 acc: 0.9341 valid loss: 0.0099 acc: 0.5747\n",
      "\n",
      "Epoch [8/59] phase: validA train loss: 0.0005 acc: 0.9773 valid loss: 0.0614 acc: 0.2882\n",
      "\n",
      "Epoch [8/59] phase: validB train loss: 0.0006 acc: 0.9717 valid loss: 0.0629 acc: 0.2885\n",
      "\n",
      "Epoch [8/59] phase: validC train loss: 0.0014 acc: 0.9400 valid loss: 0.0078 acc: 0.6336\n",
      "\n",
      "Epoch [9/59] phase: validA train loss: 0.0005 acc: 0.9783 valid loss: 0.0610 acc: 0.2883\n",
      "\n",
      "Epoch [9/59] phase: validB train loss: 0.0006 acc: 0.9709 valid loss: 0.0667 acc: 0.2870\n",
      "\n",
      "Epoch [9/59] phase: validC train loss: 0.0013 acc: 0.9423 valid loss: 0.0083 acc: 0.6147\n",
      "\n",
      "Epoch [10/59] phase: validA train loss: 0.0005 acc: 0.9787 valid loss: 0.0624 acc: 0.2886\n",
      "\n",
      "Epoch [10/59] phase: validB train loss: 0.0005 acc: 0.9724 valid loss: 0.0663 acc: 0.2863\n",
      "\n",
      "Epoch [10/59] phase: validC train loss: 0.0013 acc: 0.9422 valid loss: 0.0082 acc: 0.6067\n",
      "\n",
      "Epoch [11/59] phase: validA train loss: 0.0005 acc: 0.9797 valid loss: 0.0630 acc: 0.2861\n",
      "\n",
      "Epoch [11/59] phase: validB train loss: 0.0005 acc: 0.9734 valid loss: 0.0676 acc: 0.2873\n",
      "\n",
      "Epoch [11/59] phase: validC train loss: 0.0012 acc: 0.9447 valid loss: 0.0081 acc: 0.6291\n",
      "\n",
      "Epoch [12/59] phase: validA train loss: 0.0004 acc: 0.9798 valid loss: 0.0656 acc: 0.2890\n",
      "\n",
      "Epoch [12/59] phase: validB train loss: 0.0005 acc: 0.9733 valid loss: 0.0676 acc: 0.2864\n",
      "\n",
      "Epoch [12/59] phase: validC train loss: 0.0012 acc: 0.9451 valid loss: 0.0076 acc: 0.6485\n",
      "\n",
      "Epoch [13/59] phase: validA train loss: 0.0004 acc: 0.9812 valid loss: 0.0629 acc: 0.2903\n",
      "\n",
      "Epoch [13/59] phase: validB train loss: 0.0005 acc: 0.9742 valid loss: 0.0678 acc: 0.2868\n",
      "\n",
      "Epoch [13/59] phase: validC train loss: 0.0012 acc: 0.9471 valid loss: 0.0066 acc: 0.6871\n",
      "\n",
      "Epoch [14/59] phase: validA train loss: 0.0004 acc: 0.9807 valid loss: 0.0644 acc: 0.2890\n",
      "\n",
      "Epoch [14/59] phase: validB train loss: 0.0005 acc: 0.9766 valid loss: 0.0701 acc: 0.2886\n",
      "\n",
      "Epoch [14/59] phase: validC train loss: 0.0011 acc: 0.9497 valid loss: 0.0076 acc: 0.6525\n",
      "\n",
      "Epoch [15/59] phase: validA train loss: 0.0004 acc: 0.9828 valid loss: 0.0643 acc: 0.2887\n",
      "\n",
      "Epoch [15/59] phase: validB train loss: 0.0005 acc: 0.9755 valid loss: 0.0712 acc: 0.2871\n",
      "\n",
      "Epoch [15/59] phase: validC train loss: 0.0011 acc: 0.9513 valid loss: 0.0074 acc: 0.6630\n",
      "\n",
      "Epoch [16/59] phase: validA train loss: 0.0004 acc: 0.9835 valid loss: 0.0651 acc: 0.2896\n",
      "\n",
      "Epoch [16/59] phase: validB train loss: 0.0005 acc: 0.9758 valid loss: 0.0691 acc: 0.2862\n",
      "\n",
      "Epoch [16/59] phase: validC train loss: 0.0011 acc: 0.9507 valid loss: 0.0076 acc: 0.6571\n",
      "\n",
      "Epoch [17/59] phase: validA train loss: 0.0004 acc: 0.9828 valid loss: 0.0644 acc: 0.2878\n",
      "\n",
      "Epoch [17/59] phase: validB train loss: 0.0005 acc: 0.9757 valid loss: 0.0686 acc: 0.2880\n",
      "\n",
      "Epoch [17/59] phase: validC train loss: 0.0011 acc: 0.9519 valid loss: 0.0063 acc: 0.7144\n",
      "\n",
      "Epoch [18/59] phase: validA train loss: 0.0003 acc: 0.9844 valid loss: 0.0670 acc: 0.2897\n",
      "\n",
      "Epoch [18/59] phase: validB train loss: 0.0004 acc: 0.9767 valid loss: 0.0693 acc: 0.2863\n",
      "\n",
      "Epoch [18/59] phase: validC train loss: 0.0010 acc: 0.9542 valid loss: 0.0063 acc: 0.7134\n",
      "\n",
      "Epoch [19/59] phase: validA train loss: 0.0003 acc: 0.9853 valid loss: 0.0663 acc: 0.2900\n",
      "\n",
      "Epoch [19/59] phase: validB train loss: 0.0004 acc: 0.9779 valid loss: 0.0708 acc: 0.2874\n",
      "\n",
      "Epoch [19/59] phase: validC train loss: 0.0010 acc: 0.9559 valid loss: 0.0074 acc: 0.6729\n",
      "\n",
      "Epoch [20/59] phase: validA train loss: 0.0003 acc: 0.9850 valid loss: 0.0663 acc: 0.2885\n",
      "\n",
      "Epoch [20/59] phase: validB train loss: 0.0004 acc: 0.9781 valid loss: 0.0718 acc: 0.2838\n",
      "\n",
      "Epoch [20/59] phase: validC train loss: 0.0010 acc: 0.9552 valid loss: 0.0066 acc: 0.7101\n",
      "\n",
      "Epoch [21/59] phase: validA train loss: 0.0003 acc: 0.9849 valid loss: 0.0672 acc: 0.2878\n",
      "\n",
      "Epoch [21/59] phase: validB train loss: 0.0004 acc: 0.9776 valid loss: 0.0720 acc: 0.2844\n",
      "\n",
      "Epoch [21/59] phase: validC train loss: 0.0009 acc: 0.9570 valid loss: 0.0080 acc: 0.6557\n",
      "\n",
      "Epoch [22/59] phase: validA train loss: 0.0003 acc: 0.9855 valid loss: 0.0661 acc: 0.2893\n",
      "\n",
      "Epoch [22/59] phase: validB train loss: 0.0004 acc: 0.9784 valid loss: 0.0701 acc: 0.2860\n",
      "\n",
      "Epoch [22/59] phase: validC train loss: 0.0009 acc: 0.9581 valid loss: 0.0073 acc: 0.6824\n",
      "\n",
      "Epoch [23/59] phase: validA train loss: 0.0003 acc: 0.9867 valid loss: 0.0678 acc: 0.2888\n",
      "\n",
      "Epoch [23/59] phase: validB train loss: 0.0004 acc: 0.9796 valid loss: 0.0732 acc: 0.2869\n",
      "\n",
      "Epoch [23/59] phase: validC train loss: 0.0009 acc: 0.9580 valid loss: 0.0070 acc: 0.6983\n",
      "\n",
      "Epoch [24/59] phase: validA train loss: 0.0003 acc: 0.9855 valid loss: 0.0674 acc: 0.2890\n",
      "\n",
      "Epoch [24/59] phase: validB train loss: 0.0004 acc: 0.9802 valid loss: 0.0734 acc: 0.2869\n",
      "\n",
      "Epoch [24/59] phase: validC train loss: 0.0009 acc: 0.9586 valid loss: 0.0070 acc: 0.7054\n",
      "\n",
      "Epoch [25/59] phase: validA train loss: 0.0003 acc: 0.9874 valid loss: 0.0711 acc: 0.2897\n",
      "\n",
      "Epoch [25/59] phase: validB train loss: 0.0004 acc: 0.9800 valid loss: 0.0745 acc: 0.2860\n",
      "\n",
      "Epoch [25/59] phase: validC train loss: 0.0009 acc: 0.9597 valid loss: 0.0076 acc: 0.6820\n",
      "\n",
      "Epoch [26/59] phase: validA train loss: 0.0003 acc: 0.9867 valid loss: 0.0686 acc: 0.2888\n",
      "\n",
      "Epoch [26/59] phase: validB train loss: 0.0004 acc: 0.9798 valid loss: 0.0743 acc: 0.2860\n",
      "\n",
      "Epoch [26/59] phase: validC train loss: 0.0008 acc: 0.9603 valid loss: 0.0080 acc: 0.6710\n",
      "\n",
      "Epoch [27/59] phase: validA train loss: 0.0003 acc: 0.9864 valid loss: 0.0686 acc: 0.2884\n",
      "\n",
      "Epoch [27/59] phase: validB train loss: 0.0004 acc: 0.9802 valid loss: 0.0728 acc: 0.2851\n",
      "\n",
      "Epoch [27/59] phase: validC train loss: 0.0008 acc: 0.9607 valid loss: 0.0068 acc: 0.7142\n",
      "\n",
      "Epoch [28/59] phase: validA train loss: 0.0003 acc: 0.9873 valid loss: 0.0676 acc: 0.2884\n",
      "\n",
      "Epoch [28/59] phase: validB train loss: 0.0004 acc: 0.9799 valid loss: 0.0740 acc: 0.2859\n",
      "\n",
      "Epoch [28/59] phase: validC train loss: 0.0008 acc: 0.9634 valid loss: 0.0071 acc: 0.7097\n",
      "\n",
      "Epoch [29/59] phase: validA train loss: 0.0003 acc: 0.9880 valid loss: 0.0704 acc: 0.2900\n",
      "\n",
      "Epoch [29/59] phase: validB train loss: 0.0004 acc: 0.9802 valid loss: 0.0729 acc: 0.2852\n",
      "\n",
      "Epoch [29/59] phase: validC train loss: 0.0008 acc: 0.9645 valid loss: 0.0070 acc: 0.7117\n",
      "\n",
      "Epoch [30/59] phase: validA train loss: 0.0003 acc: 0.9884 valid loss: 0.0681 acc: 0.2902\n",
      "\n",
      "Epoch [30/59] phase: validB train loss: 0.0004 acc: 0.9811 valid loss: 0.0743 acc: 0.2882\n",
      "\n",
      "Epoch [30/59] phase: validC train loss: 0.0008 acc: 0.9636 valid loss: 0.0067 acc: 0.7226\n",
      "\n",
      "Epoch [31/59] phase: validA train loss: 0.0002 acc: 0.9890 valid loss: 0.0689 acc: 0.2896\n",
      "\n",
      "Epoch [31/59] phase: validB train loss: 0.0004 acc: 0.9813 valid loss: 0.0769 acc: 0.2876\n",
      "\n",
      "Epoch [31/59] phase: validC train loss: 0.0008 acc: 0.9654 valid loss: 0.0071 acc: 0.7258\n",
      "\n",
      "Epoch [32/59] phase: validA train loss: 0.0002 acc: 0.9891 valid loss: 0.0707 acc: 0.2892\n",
      "\n",
      "Epoch [32/59] phase: validB train loss: 0.0004 acc: 0.9817 valid loss: 0.0735 acc: 0.2871\n",
      "\n",
      "Epoch [32/59] phase: validC train loss: 0.0007 acc: 0.9650 valid loss: 0.0079 acc: 0.6996\n",
      "\n",
      "Epoch [33/59] phase: validA train loss: 0.0002 acc: 0.9893 valid loss: 0.0722 acc: 0.2886\n",
      "\n",
      "Epoch [33/59] phase: validB train loss: 0.0003 acc: 0.9826 valid loss: 0.0750 acc: 0.2879\n",
      "\n",
      "Epoch [33/59] phase: validC train loss: 0.0007 acc: 0.9653 valid loss: 0.0076 acc: 0.6962\n",
      "\n",
      "Epoch [34/59] phase: validA train loss: 0.0002 acc: 0.9892 valid loss: 0.0728 acc: 0.2893\n",
      "\n",
      "Epoch [34/59] phase: validB train loss: 0.0003 acc: 0.9824 valid loss: 0.0743 acc: 0.2871\n",
      "\n",
      "Epoch [34/59] phase: validC train loss: 0.0007 acc: 0.9658 valid loss: 0.0074 acc: 0.7145\n",
      "\n",
      "Epoch [35/59] phase: validA train loss: 0.0002 acc: 0.9897 valid loss: 0.0735 acc: 0.2883\n",
      "\n",
      "Epoch [35/59] phase: validB train loss: 0.0003 acc: 0.9821 valid loss: 0.0772 acc: 0.2860\n",
      "\n",
      "Epoch [35/59] phase: validC train loss: 0.0007 acc: 0.9666 valid loss: 0.0084 acc: 0.6950\n",
      "\n",
      "Epoch [36/59] phase: validA train loss: 0.0002 acc: 0.9895 valid loss: 0.0736 acc: 0.2875\n",
      "\n",
      "Epoch [36/59] phase: validB train loss: 0.0003 acc: 0.9819 valid loss: 0.0778 acc: 0.2840\n",
      "\n",
      "Epoch [36/59] phase: validC train loss: 0.0007 acc: 0.9672 valid loss: 0.0075 acc: 0.7199\n",
      "\n",
      "Epoch [37/59] phase: validA train loss: 0.0002 acc: 0.9906 valid loss: 0.0728 acc: 0.2881\n",
      "\n",
      "Epoch [37/59] phase: validB train loss: 0.0003 acc: 0.9808 valid loss: 0.0761 acc: 0.2849\n",
      "\n",
      "Epoch [37/59] phase: validC train loss: 0.0007 acc: 0.9676 valid loss: 0.0078 acc: 0.7097\n",
      "\n",
      "Epoch [38/59] phase: validA train loss: 0.0002 acc: 0.9904 valid loss: 0.0736 acc: 0.2870\n",
      "\n",
      "Epoch [38/59] phase: validB train loss: 0.0003 acc: 0.9838 valid loss: 0.0779 acc: 0.2844\n",
      "\n",
      "Epoch [38/59] phase: validC train loss: 0.0007 acc: 0.9690 valid loss: 0.0079 acc: 0.7093\n",
      "\n",
      "Epoch [39/59] phase: validA train loss: 0.0002 acc: 0.9895 valid loss: 0.0737 acc: 0.2887\n",
      "\n",
      "Epoch [39/59] phase: validB train loss: 0.0003 acc: 0.9820 valid loss: 0.0761 acc: 0.2882\n",
      "\n",
      "Epoch [39/59] phase: validC train loss: 0.0006 acc: 0.9694 valid loss: 0.0076 acc: 0.7232\n",
      "\n",
      "Epoch [40/59] phase: validA train loss: 0.0002 acc: 0.9908 valid loss: 0.0723 acc: 0.2890\n",
      "\n",
      "Epoch [40/59] phase: validB train loss: 0.0003 acc: 0.9821 valid loss: 0.0757 acc: 0.2889\n",
      "\n",
      "Epoch [40/59] phase: validC train loss: 0.0007 acc: 0.9706 valid loss: 0.0090 acc: 0.6890\n",
      "\n",
      "Epoch [41/59] phase: validA train loss: 0.0002 acc: 0.9915 valid loss: 0.0742 acc: 0.2891\n",
      "\n",
      "Epoch [41/59] phase: validB train loss: 0.0003 acc: 0.9822 valid loss: 0.0759 acc: 0.2842\n",
      "\n",
      "Epoch [41/59] phase: validC train loss: 0.0006 acc: 0.9693 valid loss: 0.0076 acc: 0.7334\n",
      "\n",
      "Epoch [42/59] phase: validA train loss: 0.0002 acc: 0.9913 valid loss: 0.0743 acc: 0.2877\n",
      "\n",
      "Epoch [42/59] phase: validB train loss: 0.0003 acc: 0.9840 valid loss: 0.0776 acc: 0.2860\n",
      "\n",
      "Epoch [42/59] phase: validC train loss: 0.0007 acc: 0.9687 valid loss: 0.0082 acc: 0.7106\n",
      "\n",
      "Epoch [43/59] phase: validA train loss: 0.0002 acc: 0.9911 valid loss: 0.0735 acc: 0.2893\n",
      "\n",
      "Epoch [43/59] phase: validB train loss: 0.0003 acc: 0.9827 valid loss: 0.0747 acc: 0.2873\n",
      "\n",
      "Epoch [43/59] phase: validC train loss: 0.0006 acc: 0.9705 valid loss: 0.0078 acc: 0.7317\n",
      "\n",
      "Epoch [44/59] phase: validA train loss: 0.0002 acc: 0.9905 valid loss: 0.0757 acc: 0.2875\n",
      "\n",
      "Epoch [44/59] phase: validB train loss: 0.0003 acc: 0.9844 valid loss: 0.0773 acc: 0.2869\n",
      "\n",
      "Epoch [44/59] phase: validC train loss: 0.0006 acc: 0.9723 valid loss: 0.0073 acc: 0.7372\n",
      "\n",
      "Epoch [45/59] phase: validA train loss: 0.0002 acc: 0.9918 valid loss: 0.0745 acc: 0.2874\n",
      "\n",
      "Epoch [45/59] phase: validB train loss: 0.0003 acc: 0.9826 valid loss: 0.0791 acc: 0.2841\n",
      "\n",
      "Epoch [45/59] phase: validC train loss: 0.0006 acc: 0.9706 valid loss: 0.0085 acc: 0.7158\n",
      "\n",
      "Epoch [46/59] phase: validA train loss: 0.0002 acc: 0.9933 valid loss: 0.0767 acc: 0.2886\n",
      "\n",
      "Epoch [46/59] phase: validB train loss: 0.0003 acc: 0.9843 valid loss: 0.0787 acc: 0.2859\n",
      "\n",
      "Epoch [46/59] phase: validC train loss: 0.0006 acc: 0.9709 valid loss: 0.0080 acc: 0.7228\n",
      "\n",
      "Epoch [47/59] phase: validA train loss: 0.0002 acc: 0.9918 valid loss: 0.0729 acc: 0.2868\n",
      "\n",
      "Epoch [47/59] phase: validB train loss: 0.0003 acc: 0.9840 valid loss: 0.0791 acc: 0.2873\n",
      "\n",
      "Epoch [47/59] phase: validC train loss: 0.0006 acc: 0.9720 valid loss: 0.0088 acc: 0.7088\n",
      "\n",
      "Epoch [48/59] phase: validA train loss: 0.0002 acc: 0.9920 valid loss: 0.0737 acc: 0.2871\n",
      "\n",
      "Epoch [48/59] phase: validB train loss: 0.0003 acc: 0.9845 valid loss: 0.0785 acc: 0.2873\n",
      "\n",
      "Epoch [48/59] phase: validC train loss: 0.0005 acc: 0.9736 valid loss: 0.0081 acc: 0.7281\n",
      "\n",
      "Epoch [49/59] phase: validA train loss: 0.0002 acc: 0.9929 valid loss: 0.0765 acc: 0.2873\n",
      "\n",
      "Epoch [49/59] phase: validB train loss: 0.0003 acc: 0.9858 valid loss: 0.0797 acc: 0.2871\n",
      "\n",
      "Epoch [49/59] phase: validC train loss: 0.0006 acc: 0.9738 valid loss: 0.0086 acc: 0.7153\n",
      "\n",
      "Epoch [50/59] phase: validA train loss: 0.0002 acc: 0.9928 valid loss: 0.0756 acc: 0.2861\n",
      "\n",
      "Epoch [50/59] phase: validB train loss: 0.0003 acc: 0.9848 valid loss: 0.0795 acc: 0.2859\n",
      "\n",
      "Epoch [50/59] phase: validC train loss: 0.0005 acc: 0.9742 valid loss: 0.0090 acc: 0.7202\n",
      "\n",
      "Epoch [51/59] phase: validA train loss: 0.0002 acc: 0.9926 valid loss: 0.0752 acc: 0.2894\n",
      "\n",
      "Epoch [51/59] phase: validB train loss: 0.0003 acc: 0.9850 valid loss: 0.0796 acc: 0.2860\n",
      "\n",
      "Epoch [51/59] phase: validC train loss: 0.0005 acc: 0.9744 valid loss: 0.0084 acc: 0.7317\n",
      "\n",
      "Epoch [52/59] phase: validA train loss: 0.0002 acc: 0.9926 valid loss: 0.0781 acc: 0.2851\n",
      "\n",
      "Epoch [52/59] phase: validB train loss: 0.0003 acc: 0.9853 valid loss: 0.0789 acc: 0.2823\n",
      "\n",
      "Epoch [52/59] phase: validC train loss: 0.0005 acc: 0.9745 valid loss: 0.0077 acc: 0.7412\n",
      "\n",
      "Epoch [53/59] phase: validA train loss: 0.0002 acc: 0.9919 valid loss: 0.0748 acc: 0.2873\n",
      "\n",
      "Epoch [53/59] phase: validB train loss: 0.0003 acc: 0.9853 valid loss: 0.0789 acc: 0.2854\n",
      "\n",
      "Epoch [53/59] phase: validC train loss: 0.0005 acc: 0.9754 valid loss: 0.0084 acc: 0.7268\n",
      "\n",
      "Epoch [54/59] phase: validA train loss: 0.0002 acc: 0.9930 valid loss: 0.0768 acc: 0.2857\n",
      "\n",
      "Epoch [54/59] phase: validB train loss: 0.0003 acc: 0.9872 valid loss: 0.0802 acc: 0.2885\n",
      "\n",
      "Epoch [54/59] phase: validC train loss: 0.0005 acc: 0.9754 valid loss: 0.0088 acc: 0.7301\n",
      "\n",
      "Epoch [55/59] phase: validA train loss: 0.0001 acc: 0.9942 valid loss: 0.0790 acc: 0.2847\n",
      "\n",
      "Epoch [55/59] phase: validB train loss: 0.0003 acc: 0.9848 valid loss: 0.0780 acc: 0.2854\n",
      "\n",
      "Epoch [55/59] phase: validC train loss: 0.0005 acc: 0.9777 valid loss: 0.0095 acc: 0.7106\n",
      "\n",
      "Epoch [56/59] phase: validA train loss: 0.0001 acc: 0.9940 valid loss: 0.0770 acc: 0.2875\n",
      "\n",
      "Epoch [56/59] phase: validB train loss: 0.0003 acc: 0.9861 valid loss: 0.0797 acc: 0.2873\n",
      "\n",
      "Epoch [56/59] phase: validC train loss: 0.0005 acc: 0.9737 valid loss: 0.0097 acc: 0.7036\n",
      "\n",
      "Epoch [57/59] phase: validA train loss: 0.0001 acc: 0.9942 valid loss: 0.0783 acc: 0.2882\n",
      "\n",
      "Epoch [57/59] phase: validB train loss: 0.0003 acc: 0.9851 valid loss: 0.0798 acc: 0.2872\n",
      "\n",
      "Epoch [57/59] phase: validC train loss: 0.0005 acc: 0.9780 valid loss: 0.0094 acc: 0.7250\n",
      "\n",
      "Epoch [58/59] phase: validA train loss: 0.0001 acc: 0.9942 valid loss: 0.0798 acc: 0.2870\n",
      "\n",
      "Epoch [58/59] phase: validB train loss: 0.0003 acc: 0.9861 valid loss: 0.0818 acc: 0.2852\n",
      "\n",
      "Epoch [58/59] phase: validC train loss: 0.0005 acc: 0.9772 valid loss: 0.0089 acc: 0.7271\n",
      "\n",
      "Epoch [59/59] phase: validA train loss: 0.0001 acc: 0.9940 valid loss: 0.0779 acc: 0.2881\n",
      "\n",
      "Epoch [59/59] phase: validB train loss: 0.0003 acc: 0.9860 valid loss: 0.0819 acc: 0.2878\n",
      "\n",
      "Epoch [59/59] phase: validC train loss: 0.0005 acc: 0.9783 valid loss: 0.0088 acc: 0.7323\n",
      "\n",
      "Best val Acc: 0.741200\n",
      "Training time:  28.085742 minutes\n"
     ]
    }
   ],
   "source": [
    "### Run decentralized pairwised knowledge transfer\n",
    "\n",
    "logging.info(\"#### bn acti conv and fc - unlimited bn - adam learning rate 0.001 - scheduler 20 - opt adamw ####\")\n",
    "\n",
    "start_time = time.time()\n",
    "model = train_model(dloaders, model, criterion, optimizer, exp_lr_scheduler, num_epochs=60)\n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4419,\n",
       " 0.4953,\n",
       " 0.4955,\n",
       " 0.5026,\n",
       " 0.5558000000000001,\n",
       " 0.5622,\n",
       " 0.5842,\n",
       " 0.5747,\n",
       " 0.6336,\n",
       " 0.6147,\n",
       " 0.6067,\n",
       " 0.6291,\n",
       " 0.6485000000000001,\n",
       " 0.6871,\n",
       " 0.6525000000000001,\n",
       " 0.663,\n",
       " 0.6571,\n",
       " 0.7144,\n",
       " 0.7134,\n",
       " 0.6729,\n",
       " 0.7101000000000001,\n",
       " 0.6557000000000001,\n",
       " 0.6824,\n",
       " 0.6983,\n",
       " 0.7054,\n",
       " 0.682,\n",
       " 0.671,\n",
       " 0.7142000000000001,\n",
       " 0.7097,\n",
       " 0.7117,\n",
       " 0.7226,\n",
       " 0.7258,\n",
       " 0.6996,\n",
       " 0.6962,\n",
       " 0.7145,\n",
       " 0.6950000000000001,\n",
       " 0.7199,\n",
       " 0.7097,\n",
       " 0.7093,\n",
       " 0.7232000000000001,\n",
       " 0.6890000000000001,\n",
       " 0.7334,\n",
       " 0.7106,\n",
       " 0.7317,\n",
       " 0.7372000000000001,\n",
       " 0.7158,\n",
       " 0.7228,\n",
       " 0.7088,\n",
       " 0.7281000000000001,\n",
       " 0.7153,\n",
       " 0.7202000000000001,\n",
       " 0.7317,\n",
       " 0.7412000000000001,\n",
       " 0.7268,\n",
       " 0.7301000000000001,\n",
       " 0.7106,\n",
       " 0.7036,\n",
       " 0.7250000000000001,\n",
       " 0.7271000000000001,\n",
       " 0.7323000000000001]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ground - Generate confusion matrices of TP,FP,FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### without 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(10, 10)\n",
    "for inputs, labels in dloaders['validC']:\n",
    "    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "    outputs = model['validC'](inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[548.,   2.,  10.,  12.,   2.,   1., 389.,   0.,  34.,   2.],\n",
      "        [  4., 951.,   3.,   3.,   4.,   2.,  18.,   0.,  13.,   2.],\n",
      "        [  3.,   1., 556.,   3., 156.,   1., 262.,   0.,  15.,   3.],\n",
      "        [ 10.,   8.,   7., 644.,  39.,  12., 229.,   0.,  30.,  21.],\n",
      "        [  0.,   0.,  26.,  14., 651.,   0., 285.,   0.,  22.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 909.,   0.,  59.,   9.,  23.],\n",
      "        [ 30.,   0.,  25.,   7.,  23.,   1., 891.,   0.,  21.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,  13.,   0., 938.,   1.,  48.],\n",
      "        [  0.,   0.,   0.,   0.,   1.,   2.,   3.,   0., 994.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   2.,   1.,  18.,   0., 979.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(10, 10)\n",
    "for inputs, labels in dloaders['validC']:\n",
    "    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "    outputs = model['validC'](inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[351.,   1.,  17.,  12.,   0.,   0., 605.,   0.,  13.,   1.],\n",
      "        [  0., 947.,   1.,   5.,   2.,   1.,  40.,   0.,   4.,   0.],\n",
      "        [  1.,   1., 551.,   1.,  74.,   0., 366.,   0.,   6.,   0.],\n",
      "        [  8.,   9.,   8., 525.,  39.,   0., 392.,   0.,  15.,   4.],\n",
      "        [  0.,   0.,  70.,   5., 485.,   0., 431.,   0.,   8.,   1.],\n",
      "        [  0.,   0.,   8.,   0.,   0., 731.,   3., 143.,  78.,  37.],\n",
      "        [ 14.,   1.,  19.,   3.,  17.,   0., 927.,   0.,  19.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   4.,   0., 836.,  31., 129.],\n",
      "        [  2.,   0.,   1.,   0.,   0.,   0.,  15.,   1., 980.,   1.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   1.,   1.,   8.,   0., 990.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with knowledge transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(10, 10)\n",
    "for inputs, labels in dloaders['validC']:\n",
    "    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "    outputs = model['validC'](inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[551.,   3.,  38.,  20.,   1.,   1., 366.,   0.,  20.,   0.],\n",
      "        [  0., 949.,  18.,   6.,   5.,   0.,  18.,   0.,   4.,   0.],\n",
      "        [  8.,   1., 657.,   4.,  48.,   0., 253.,   0.,  29.,   0.],\n",
      "        [  8.,   9.,  70., 617.,  20.,   2., 263.,   0.,  11.,   0.],\n",
      "        [  0.,   2., 139.,  14., 495.,   0., 318.,   0.,  32.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 872.,   0.,  60.,  22.,  46.],\n",
      "        [ 55.,   2.,  77.,   9.,  30.,   2., 801.,   0.,  24.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,  36.,   0., 842.,  10., 112.],\n",
      "        [  0.,   0.,   2.,   2.,   0.,   0.,  14.,   0., 979.,   3.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   1.,   0.,   9.,   1., 989.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
