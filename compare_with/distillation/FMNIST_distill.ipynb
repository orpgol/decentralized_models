{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "from scipy.stats import entropy, ks_2samp\n",
    "from scipy.special import kl_div\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# %load_ext tensorboard\n",
    "\n",
    "import traceback\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logname = 'logs/decentralized_distil'\n",
    "logging.basicConfig(filename=logname,\n",
    "                            filemode='a',\n",
    "                            format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                            datefmt='%H:%M:%S',\n",
    "                            level=logging.DEBUG)\n",
    "\n",
    "logging.info(\"Running Decentralized Learning test\")\n",
    "\n",
    "logger = logging.getLogger('Decentralized_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbdf6b743f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "# Standard transformations for improving CIFAR10. \n",
    "\n",
    "# Transformations A\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Transformations B\n",
    "RC   = transforms.RandomCrop(32, padding=4)\n",
    "RHF  = transforms.RandomHorizontalFlip()\n",
    "RVF  = transforms.RandomVerticalFlip()\n",
    "NRM  = transforms.Normalize([0.5], [0.5])\n",
    "TT   = transforms.ToTensor()\n",
    "TPIL = transforms.ToPILImage()\n",
    "\n",
    "# Transforms object for trainset with augmentation\n",
    "transform_with_aug = transforms.Compose([TPIL, RC, RHF, TT, NRM])\n",
    "# Transforms object for testset with NO augmentation\n",
    "transform_no_aug   = transforms.Compose([TPIL, TT, NRM])\n",
    "\n",
    "# Downloading/Louding CIFAR10 data\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../../../data/fmnist', train=True,\n",
    "                                        download=True, transform=transform_with_aug)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='../../../data/fmnist', train=False,\n",
    "                                       download=True, transform=transform_no_aug)\n",
    "\n",
    "\n",
    "classDict = {'T-shirt/top':0, 'Trouser':1, 'Pullover':2, 'Dress':3, 'Coat':4, 'Sandal':5, 'Shirt':6, 'Sneaker':7, 'Bag':8, 'Ankle boot':9}\n",
    "\n",
    "# Separating trainset/testset data/label\n",
    "x_train  = trainset.data\n",
    "x_test   = testset.data\n",
    "y_train  = trainset.targets\n",
    "y_test   = testset.targets\n",
    "\n",
    "# Define a function to separate CIFAR classes by class index\n",
    "\n",
    "def get_class_i(x, y, i):\n",
    "    \"\"\"\n",
    "    x: trainset.train_data or testset.test_data\n",
    "    y: trainset.train_labels or testset.test_labels\n",
    "    i: class label, a number between 0 to 9\n",
    "    return: x_i\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array\n",
    "    y = np.array(y)\n",
    "    # Locate position of labels that equal to i\n",
    "    pos_i = np.argwhere(y == i)\n",
    "    # Convert the result into a 1-D list\n",
    "    pos_i = list(pos_i[:,0])\n",
    "    # Collect all data that match the desired label\n",
    "    x_i = [x[j] for j in pos_i]\n",
    "    \n",
    "    return x_i\n",
    "\n",
    "class DatasetMaker(Dataset):\n",
    "    def __init__(self, datasets, transformFunc = transform_no_aug):\n",
    "        \"\"\"\n",
    "        datasets: a list of get_class_i outputs, i.e. a list of list of images for selected classes\n",
    "        \"\"\"\n",
    "        self.datasets = datasets\n",
    "        self.lengths  = [len(d) for d in self.datasets]\n",
    "        self.transformFunc = transformFunc\n",
    "    def __getitem__(self, i):\n",
    "        class_label, index_wrt_class = self.index_of_which_bin(self.lengths, i)\n",
    "        img = self.datasets[class_label][index_wrt_class]\n",
    "        img = self.transformFunc(img)\n",
    "        return img, class_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lengths)\n",
    "    \n",
    "    def index_of_which_bin(self, bin_sizes, absolute_index, verbose=False):\n",
    "        \"\"\"\n",
    "        Given the absolute index, returns which bin it falls in and which element of that bin it corresponds to.\n",
    "        \"\"\"\n",
    "        # Which class/bin does i fall into?\n",
    "        accum = np.add.accumulate(bin_sizes)\n",
    "        if verbose:\n",
    "            print(\"accum =\", accum)\n",
    "        bin_index  = len(np.argwhere(accum <= absolute_index))\n",
    "        if verbose:\n",
    "            print(\"class_label =\", bin_index)\n",
    "        # Which element of the fallent class/bin does i correspond to?\n",
    "        index_wrt_class = absolute_index - np.insert(accum, 0, 0)[bin_index]\n",
    "        if verbose:\n",
    "            print(\"index_wrt_class =\", index_wrt_class)\n",
    "\n",
    "        return bin_index, index_wrt_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we are saving a fraction\n",
    "frac = int(len(x_train) * 0.05)\n",
    "x_reserve = x_train[:frac]\n",
    "y_reserve = y_train[:frac]\n",
    "x_train = x_train[frac:]\n",
    "y_train = y_train[frac:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Usage ================== #\n",
    "\n",
    "# \n",
    "trainset1 = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train, y_train, classDict['T-shirt/top']), \n",
    "         get_class_i(x_train, y_train, classDict['Trouser']), \n",
    "         get_class_i(x_train, y_train, classDict['Pullover']),\n",
    "        [],[],[],[],[],[],[]],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset2 = \\\n",
    "    DatasetMaker(\n",
    "        [[],[],[],\n",
    "         get_class_i(x_train, y_train, classDict['Dress']), \n",
    "         get_class_i(x_train, y_train, classDict['Coat']), \n",
    "         get_class_i(x_train, y_train, classDict['Sandal']), \n",
    "         [],[],[],[]],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset3 = \\\n",
    "    DatasetMaker(\n",
    "        [[],[],[],[],[],[],\n",
    "         get_class_i(x_train, y_train, classDict['Shirt']), \n",
    "         get_class_i(x_train, y_train, classDict['Sneaker']), \n",
    "         get_class_i(x_train, y_train, classDict['Bag']), \n",
    "         get_class_i(x_train, y_train, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "trainset4 = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_train, y_train, classDict['T-shirt/top']), \n",
    "         get_class_i(x_train, y_train, classDict['Trouser']), \n",
    "         get_class_i(x_train, y_train, classDict['Pullover']), \n",
    "         get_class_i(x_train, y_train, classDict['Dress']), \n",
    "         get_class_i(x_train, y_train, classDict['Coat']),\n",
    "         get_class_i(x_train, y_train, classDict['Sandal']), \n",
    "         get_class_i(x_train, y_train, classDict['Shirt']), \n",
    "         get_class_i(x_train, y_train, classDict['Sneaker']), \n",
    "         get_class_i(x_train, y_train, classDict['Bag']), \n",
    "         get_class_i(x_train, y_train, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "reserved = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_reserve, y_reserve, classDict['T-shirt/top']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Trouser']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Pullover']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Dress']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Coat']),\n",
    "         get_class_i(x_reserve, y_reserve, classDict['Sandal']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Shirt']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Sneaker']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Bag']), \n",
    "         get_class_i(x_reserve, y_reserve, classDict['Ankle boot'])],\n",
    "        transform_with_aug\n",
    "    )\n",
    "# reservedB = \\\n",
    "#     DatasetMaker(\n",
    "#         [get_class_i(x_reserveB, y_reserveB, classDict['plane']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['car']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['bird']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['cat']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['deer']),\n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['dog']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['frog']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['horse']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['ship']), \n",
    "#          get_class_i(x_reserveB, y_reserveB, classDict['truck'])],\n",
    "#         transform_with_aug\n",
    "#     )\n",
    "testset  = \\\n",
    "    DatasetMaker(\n",
    "        [get_class_i(x_test, y_test, classDict['T-shirt/top']), \n",
    "         get_class_i(x_test, y_test, classDict['Trouser']), \n",
    "         get_class_i(x_test, y_test, classDict['Pullover']), \n",
    "         get_class_i(x_test, y_test, classDict['Dress']), \n",
    "         get_class_i(x_test, y_test, classDict['Coat']),\n",
    "         get_class_i(x_test, y_test, classDict['Sandal']), \n",
    "         get_class_i(x_test, y_test, classDict['Shirt']), \n",
    "         get_class_i(x_test, y_test, classDict['Sneaker']), \n",
    "         get_class_i(x_test, y_test, classDict['Bag']), \n",
    "         get_class_i(x_test, y_test, classDict['Ankle boot'])],\n",
    "        transform_no_aug\n",
    "    )\n",
    "\n",
    "superset = torch.utils.data.ConcatDataset([trainset3,reserved])\n",
    "supersetB = torch.utils.data.ConcatDataset([trainset2,reserved])\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': False}\n",
    "\n",
    "# Create datasetLoaders from trainset and testset\n",
    "trainsetLoader1   = DataLoader(trainset1, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader2   = DataLoader(trainset2, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader3   = DataLoader(trainset3, batch_size=128, shuffle=True , **kwargs)\n",
    "trainsetLoader4   = DataLoader(trainset4, batch_size=128, shuffle=True , **kwargs)\n",
    "reservedLoader    = DataLoader(superset, batch_size=128, shuffle=True , **kwargs)\n",
    "reservedLoaderB   = DataLoader(supersetB, batch_size=128, shuffle=True , **kwargs)\n",
    "testsetLoader     = DataLoader(testset , batch_size=128, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.3    # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = next(iter(reservedLoader))\n",
    "imshow(torchvision.utils.make_grid(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(device)\n",
    "\n",
    "# device2 = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decentralized model using reserved data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a knowledge transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(m):\n",
    "    if isinstance(m, (torch.nn.Linear)):\n",
    "        torch.nn.init.sparse_(m.weight, sparsity=0.33)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (torch.nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "class Decenter(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter, self).__init__()\n",
    "        if len(shape) == 1:\n",
    "            shape = shape[0]\n",
    "            self.dim = 0\n",
    "        elif len(shape) == 2:\n",
    "            shape = shape[1]\n",
    "            self.dim = 1\n",
    "        self.translation = torch.nn.Sequential(\n",
    "#             torch.nn.Tanh(),\n",
    "            torch.nn.Linear(shape*3, shape)\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source1, source2, target):\n",
    "        x = torch.cat((source1, source2, target), self.dim)\n",
    "#         x = torch.cat((torch.flatten(source), torch.flatten(target)), 0)\n",
    "#         x = torch.add(torch.flatten(source).to(\"cpu\"), torch.flatten(target).to(\"cpu\"))\n",
    "        res = self.translation(x)\n",
    "#         res = res.reshape(target.shape)\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class Interpolate(torch.nn.Module):\n",
    "    def __init__(self, size, mode):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = torch.nn.functional.interpolate\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n",
    "        return x\n",
    "    \n",
    "class Reshape(torch.nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Decenter_pooled(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter_pooled, self).__init__()\n",
    "        self.shape = shape\n",
    "        self.translation = torch.nn.Sequential(\n",
    "#             torch.nn.BatchNorm2d(channels_out),\n",
    "#             torch.nn.AdaptiveAvgPool2d(1),\n",
    "            Interpolate(size=1, mode='bilinear'),\n",
    "            Reshape(shape[0], shape[1]*2),\n",
    "            torch.nn.Linear(shape[1]*2, shape[1]*shape[-1]*shape[-1]),\n",
    "#             Reshape(shape[0], shape[1] ,1 ,1),\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        x = torch.cat((source, target), 1)\n",
    "        res = self.translation(x)\n",
    "        res = res.view(self.shape[0], self.shape[1], self.shape[2], self.shape[3])\n",
    "        return res\n",
    "    \n",
    "    \n",
    "class Decenter_conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, shape):\n",
    "        super(Decenter_conv, self).__init__()\n",
    "        self.shape = shape\n",
    "        channels_in = shape[1]*3\n",
    "        channels_out = shape[1]\n",
    "        self.translation = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm2d(channels_in),\n",
    "            torch.nn.Conv2d(channels_in, channels_out, 3, stride=1, padding=1)\n",
    "#             torch.nn.ConvTranspose2d(channels_in, channels_out, 3, stride=1, padding=1)\n",
    "#             torch.nn.Linear(shape[0]*2, shape[0]*4),\n",
    "#             torch.nn.Dropout(p=0.5),\n",
    "#             torch.nn.Linear(shape[0]*2, shape[0]),\n",
    "#             torch.nn.AdaptiveAvgPool2d((shape[-2],shape[-1])),\n",
    "#             torch.nn.Conv2d(channels_out, channels_out, 3, stride=1, padding=1)\n",
    "\n",
    "        )\n",
    "        self.translation2 = torch.nn.Sequential(\n",
    "            torch.nn.AdaptiveMaxPool2d((shape[-2],shape[-1])),\n",
    "        )\n",
    "\n",
    "#         self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, source1, source2, target):\n",
    "        x = torch.cat((source1, source2, target), 1)\n",
    "#         x = x.reshape(-1, x.shape[0])\n",
    "#         x = torch.cat((torch.flatten(source), torch.flatten(target)), 0)\n",
    "#         x = torch.add(torch.flatten(source).to(\"cpu\"), torch.flatten(target).to(\"cpu\"))\n",
    "        res = self.translation(x)\n",
    "#         res = res.reshape(self.shape)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_dict: dict = {}\n",
    "def fc_hook(layer_name, grad_input, grad_output): \n",
    "    if layer_name not in grad_dict:\n",
    "        grad_dict[layer_name] = {}\n",
    "        grad_dict[layer_name][\"grad_input\"] = []\n",
    "        grad_dict[layer_name][\"grad_output\"] = []\n",
    "        grad_dict[layer_name][\"labels\"] = []\n",
    "        \n",
    "#     print(grad_input)\n",
    "#     print(grad_output)\n",
    "    grad_dict[layer_name][\"grad_input\"].append(grad_input[0].cpu().numpy())\n",
    "    grad_dict[layer_name][\"grad_output\"].append(grad_output[0].cpu().numpy())\n",
    "    \n",
    "# def reserve_step(source, target):\n",
    "    \n",
    "reslist = []\n",
    "matlst = []\n",
    "fclst = []\n",
    "\n",
    "options = {0: ['trainA', 'validA','reservedAB'], \n",
    "           1: ['trainB','validB','reservedBA'],\n",
    "           2: ['trainC','validC','reservedCA','validC']}\n",
    "\n",
    "def train_model(dataloders, model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "#     writer = SummaryWriter('runs/') \n",
    "\n",
    "    since = time.time()\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    best_model_wts = 0.0\n",
    "    best_acc = 0.0\n",
    "    dataset_sizes = {'trainA': len(dataloders['trainA'].sampler),\n",
    "                     'trainB': len(dataloders['trainB'].sampler),\n",
    "                     'trainC': len(dataloders['trainC'].sampler),\n",
    "                     'reservedA': len(dataloders['reservedA'].sampler),\n",
    "                     'reservedB': len(dataloders['reservedB'].sampler),\n",
    "                     'reservedCA': len(dataloders['reservedCA'].sampler),\n",
    "                     'reservedAB': len(dataloders['reservedAB'].sampler),\n",
    "                     'reservedBA': len(dataloders['reservedBA'].sampler),\n",
    "                     'validA': len(dataloders['validA'].sampler),\n",
    "                     'validB': len(dataloders['validB'].sampler),\n",
    "                     'validC': len(dataloders['validC'].sampler)}\n",
    "\n",
    "    i = 0\n",
    "    ivc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['trainA', 'validA','trainB','validB','trainC','validC','reservedCA','validC']:\n",
    "#         choice = np.random.choice(range(3), replace=False)\n",
    "#         for phase in options[choice]:\n",
    "            if phase not in ['validA','validB','validC']:\n",
    "                model[phase].train(True)\n",
    "            else:\n",
    "                model['trainA'].train(False)\n",
    "                model['trainB'].train(False)\n",
    "                model['trainC'].train(False)\n",
    "            \n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0                    \n",
    "                    \n",
    "            for inputs, labels in dataloders[phase]:\n",
    "                if use_gpu:\n",
    "                    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                optimizer[phase].zero_grad()\n",
    "\n",
    "                outputs = model[phase](inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                if phase in ['reservedCA','reservedAB','reservedBA']:\n",
    "                    loss_a = criterion['trainC'](outputs, labels)\n",
    "                    if phase == 'reservedCA':\n",
    "                        outputs2 = model['trainA'](inputs)\n",
    "                        outputs3 = model['trainB'](inputs)\n",
    "                    elif phase == 'reservedAB':\n",
    "                        outputs2 = model['trainC'](inputs)\n",
    "                        outputs3 = model['trainB'](inputs)\n",
    "                    elif phase == 'reservedBA':\n",
    "                        outputs2 = model['trainC'](inputs)\n",
    "                        outputs3 = model['trainA'](inputs)\n",
    "    \n",
    "                    sm = torch.nn.Softmax(dim=1)\n",
    "                    outputs = sm(outputs)\n",
    "                    outputs2 = sm(outputs2)\n",
    "                    outputs3 = sm(outputs3)\n",
    "                    loss_b = criterion[phase](outputs, outputs2)\n",
    "                    loss_c = criterion[phase](outputs, outputs3)\n",
    "                    loss = (loss_a + (loss_b + loss_c)/2)/2\n",
    "                else:\n",
    "                    loss = criterion[phase](outputs, labels)\n",
    "\n",
    "                if phase not in ['validA','validB','validC']:\n",
    "                    loss.backward()\n",
    "                    optimizer[phase].step()\n",
    "                                    \n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase in ['validA','validB','validC']:\n",
    "                scheduler[phase].step(running_loss)\n",
    "                \n",
    "            if phase not in ['validA','validB','validC']:\n",
    "                train_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                train_epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            else:\n",
    "                valid_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                valid_epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print('Epoch [{}/{}] phase: {} train loss: {:.4f} acc: {:.4f} ' \n",
    "                      'valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                        epoch, num_epochs - 1,\n",
    "                        phase,\n",
    "                        train_epoch_loss, train_epoch_acc, \n",
    "                        valid_epoch_loss, valid_epoch_acc))\n",
    "                print() \n",
    "                logger.info('Epoch [{}/{}] phase: {} train loss: {:.4f} acc: {:.4f} ' \n",
    "                      'valid loss: {:.4f} acc: {:.4f}'.format(\n",
    "                        epoch, num_epochs - 1,\n",
    "                        phase,\n",
    "                        train_epoch_loss, train_epoch_acc, \n",
    "                        valid_epoch_loss, valid_epoch_acc))\n",
    "                \n",
    "                # Writing to tensorboard\n",
    "                if phase == 'validC':\n",
    "                    ivc += 1\n",
    "                    if ivc == 2:\n",
    "#                         writer.add_histogram('distribution centers/our_full_mesh', outputs, i)\n",
    "\n",
    "#                         writer.add_scalar('train/loss_our_full_mesh', train_epoch_loss, epoch)\n",
    "#                         writer.add_scalar('train/accuracy_our_full_mesh', train_epoch_acc, epoch)\n",
    "\n",
    "#                         writer.add_scalar('valid/loss_our_full_mesh', valid_epoch_loss, epoch)\n",
    "#                         writer.add_scalar('valid/accuracy_our_full_mesh', valid_epoch_acc, epoch)\n",
    "                        reslist.append(valid_epoch_acc.item())\n",
    "                        ivc = 0\n",
    "\n",
    "                \n",
    "            if phase in ['validA','validB','validC'] and valid_epoch_acc > best_acc:\n",
    "                best_acc = valid_epoch_acc\n",
    "                best_model_wts = model[phase].state_dict()\n",
    "\n",
    "            i+=1\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    logger.info('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "#     writer.close()\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnetA = models.resnet50(pretrained=True)\n",
    "resnetB = models.resnet50(pretrained=True)\n",
    "resnetC = models.resnet50(pretrained=True)\n",
    "# freeze all model parameters\n",
    "# for param in resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# new final layer with 10 classes\n",
    "num_ftrsA = resnetA.fc.in_features\n",
    "resnetA.fc = torch.nn.Linear(num_ftrsA, 10)\n",
    "resnetA.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrsB = resnetB.fc.in_features\n",
    "resnetB.fc = torch.nn.Linear(num_ftrsB, 10)\n",
    "resnetB.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrsC = resnetC.fc.in_features\n",
    "resnetC.fc = torch.nn.Linear(num_ftrsC, 10)\n",
    "resnetC.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "def fc_backward_hook(module, grad_input, grad_output):  # module is Linear in this case. Ignored.\n",
    "        fc_hook(\"fc\", grad_input, grad_output)\n",
    "resnetA.fc_hook_handle = resnetA.fc.register_backward_hook(fc_backward_hook)\n",
    "resnetB.fc_hook_handle = resnetB.fc.register_backward_hook(fc_backward_hook)\n",
    "resnetC.fc_hook_handle = resnetC.fc.register_backward_hook(fc_backward_hook)\n",
    "\n",
    "\n",
    "def roc_auc_score_micro(y_pred_proba, y_true):\n",
    "    y_pred_proba = y_pred_proba.detach().cpu()\n",
    "    y_true = y_true.detach().cpu()\n",
    "    return metrics.roc_auc_score(\n",
    "        label_binarize(y_true, classes=list(range(y_pred_proba.shape[1]))).ravel(),\n",
    "        y_pred_proba.flatten())\n",
    "\n",
    "\n",
    "resnetA = resnetA.to(device)\n",
    "resnetB = resnetB.to(device)\n",
    "resnetC = resnetC.to(device)\n",
    "\n",
    "criterionA = torch.nn.CrossEntropyLoss()\n",
    "# criterionB = torch.nn.CrossEntropyLoss()\n",
    "# criterionA = torch.nn.KLDivLoss()\n",
    "criterionB = torch.nn.KLDivLoss(reduction = 'batchmean')\n",
    "# criterionB = torch.nn.KLDivLoss(reduction = 'mean')\n",
    "# criterionB = torch.nn.MSELoss()\n",
    "optimizerA = torch.optim.SGD(resnetA.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizerB = torch.optim.SGD(resnetB.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizerC = torch.optim.SGD(resnetC.parameters(), lr=0.01, momentum=0.9)\n",
    "# optimizerA = torch.optim.AdamW(resnetA.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# optimizerB = torch.optim.AdamW(resnetB.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "# optimizerC = torch.optim.AdamW(resnetC.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# exp_lr_schedulerA = lr_scheduler.StepLR(optimizerA, step_size=5, gamma=0.01)\n",
    "# exp_lr_schedulerB = lr_scheduler.StepLR(optimizerB, step_size=5, gamma=0.01)\n",
    "# exp_lr_schedulerC = lr_scheduler.StepLR(optimizerC, step_size=5, gamma=0.2)\n",
    "exp_lr_schedulerA = lr_scheduler.ReduceLROnPlateau(optimizerA, 'min', factor=0.90, patience=500)\n",
    "exp_lr_schedulerB = lr_scheduler.ReduceLROnPlateau(optimizerB, 'min', factor=0.90, patience=500)\n",
    "exp_lr_schedulerC = lr_scheduler.ReduceLROnPlateau(optimizerC, 'min', factor=0.90, patience=500)\n",
    "\n",
    "\n",
    "def hwout(Hin, padding, dilation, kernel_size, stride):\n",
    "    return (Hin + 2 * padding - dilation * (kernel_size-1) - 1)/stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_layer = 0\n",
    "max_neurons = 0\n",
    "for prm in resnetC.named_parameters():\n",
    "    num_ftr = np.prod(prm[1].shape)\n",
    "    if num_ftr > max_neurons:\n",
    "         max_neurons = num_ftr\n",
    "         max_layer = prm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'bn1.weight',\n",
       " 'bn1.bias',\n",
       " 'layer1.0.conv1.weight',\n",
       " 'layer1.0.bn1.weight',\n",
       " 'layer1.0.bn1.bias',\n",
       " 'layer1.0.conv2.weight',\n",
       " 'layer1.0.bn2.weight',\n",
       " 'layer1.0.bn2.bias',\n",
       " 'layer1.0.conv3.weight',\n",
       " 'layer1.0.bn3.weight',\n",
       " 'layer1.0.bn3.bias',\n",
       " 'layer1.0.downsample.0.weight',\n",
       " 'layer1.0.downsample.1.weight',\n",
       " 'layer1.0.downsample.1.bias',\n",
       " 'layer1.1.conv1.weight',\n",
       " 'layer1.1.bn1.weight',\n",
       " 'layer1.1.bn1.bias',\n",
       " 'layer1.1.conv2.weight',\n",
       " 'layer1.1.bn2.weight',\n",
       " 'layer1.1.bn2.bias',\n",
       " 'layer1.1.conv3.weight',\n",
       " 'layer1.1.bn3.weight',\n",
       " 'layer1.1.bn3.bias',\n",
       " 'layer1.2.conv1.weight',\n",
       " 'layer1.2.bn1.weight',\n",
       " 'layer1.2.bn1.bias',\n",
       " 'layer1.2.conv2.weight',\n",
       " 'layer1.2.bn2.weight',\n",
       " 'layer1.2.bn2.bias',\n",
       " 'layer1.2.conv3.weight',\n",
       " 'layer1.2.bn3.weight',\n",
       " 'layer1.2.bn3.bias',\n",
       " 'layer2.0.conv1.weight',\n",
       " 'layer2.0.bn1.weight',\n",
       " 'layer2.0.bn1.bias',\n",
       " 'layer2.0.conv2.weight',\n",
       " 'layer2.0.bn2.weight',\n",
       " 'layer2.0.bn2.bias',\n",
       " 'layer2.0.conv3.weight',\n",
       " 'layer2.0.bn3.weight',\n",
       " 'layer2.0.bn3.bias',\n",
       " 'layer2.0.downsample.0.weight',\n",
       " 'layer2.0.downsample.1.weight',\n",
       " 'layer2.0.downsample.1.bias',\n",
       " 'layer2.1.conv1.weight',\n",
       " 'layer2.1.bn1.weight',\n",
       " 'layer2.1.bn1.bias',\n",
       " 'layer2.1.conv2.weight',\n",
       " 'layer2.1.bn2.weight',\n",
       " 'layer2.1.bn2.bias',\n",
       " 'layer2.1.conv3.weight',\n",
       " 'layer2.1.bn3.weight',\n",
       " 'layer2.1.bn3.bias',\n",
       " 'layer2.2.conv1.weight',\n",
       " 'layer2.2.bn1.weight',\n",
       " 'layer2.2.bn1.bias',\n",
       " 'layer2.2.conv2.weight',\n",
       " 'layer2.2.bn2.weight',\n",
       " 'layer2.2.bn2.bias',\n",
       " 'layer2.2.conv3.weight',\n",
       " 'layer2.2.bn3.weight',\n",
       " 'layer2.2.bn3.bias',\n",
       " 'layer2.3.conv1.weight',\n",
       " 'layer2.3.bn1.weight',\n",
       " 'layer2.3.bn1.bias',\n",
       " 'layer2.3.conv2.weight',\n",
       " 'layer2.3.bn2.weight',\n",
       " 'layer2.3.bn2.bias',\n",
       " 'layer2.3.conv3.weight',\n",
       " 'layer2.3.bn3.weight',\n",
       " 'layer2.3.bn3.bias',\n",
       " 'layer3.0.conv1.weight',\n",
       " 'layer3.0.bn1.weight',\n",
       " 'layer3.0.bn1.bias',\n",
       " 'layer3.0.conv2.weight',\n",
       " 'layer3.0.bn2.weight',\n",
       " 'layer3.0.bn2.bias',\n",
       " 'layer3.0.conv3.weight',\n",
       " 'layer3.0.bn3.weight',\n",
       " 'layer3.0.bn3.bias',\n",
       " 'layer3.0.downsample.0.weight',\n",
       " 'layer3.0.downsample.1.weight',\n",
       " 'layer3.0.downsample.1.bias',\n",
       " 'layer3.1.conv1.weight',\n",
       " 'layer3.1.bn1.weight',\n",
       " 'layer3.1.bn1.bias',\n",
       " 'layer3.1.conv2.weight',\n",
       " 'layer3.1.bn2.weight',\n",
       " 'layer3.1.bn2.bias',\n",
       " 'layer3.1.conv3.weight',\n",
       " 'layer3.1.bn3.weight',\n",
       " 'layer3.1.bn3.bias',\n",
       " 'layer3.2.conv1.weight',\n",
       " 'layer3.2.bn1.weight',\n",
       " 'layer3.2.bn1.bias',\n",
       " 'layer3.2.conv2.weight',\n",
       " 'layer3.2.bn2.weight',\n",
       " 'layer3.2.bn2.bias',\n",
       " 'layer3.2.conv3.weight',\n",
       " 'layer3.2.bn3.weight',\n",
       " 'layer3.2.bn3.bias',\n",
       " 'layer3.3.conv1.weight',\n",
       " 'layer3.3.bn1.weight',\n",
       " 'layer3.3.bn1.bias',\n",
       " 'layer3.3.conv2.weight',\n",
       " 'layer3.3.bn2.weight',\n",
       " 'layer3.3.bn2.bias',\n",
       " 'layer3.3.conv3.weight',\n",
       " 'layer3.3.bn3.weight',\n",
       " 'layer3.3.bn3.bias',\n",
       " 'layer3.4.conv1.weight',\n",
       " 'layer3.4.bn1.weight',\n",
       " 'layer3.4.bn1.bias',\n",
       " 'layer3.4.conv2.weight',\n",
       " 'layer3.4.bn2.weight',\n",
       " 'layer3.4.bn2.bias',\n",
       " 'layer3.4.conv3.weight',\n",
       " 'layer3.4.bn3.weight',\n",
       " 'layer3.4.bn3.bias',\n",
       " 'layer3.5.conv1.weight',\n",
       " 'layer3.5.bn1.weight',\n",
       " 'layer3.5.bn1.bias',\n",
       " 'layer3.5.conv2.weight',\n",
       " 'layer3.5.bn2.weight',\n",
       " 'layer3.5.bn2.bias',\n",
       " 'layer3.5.conv3.weight',\n",
       " 'layer3.5.bn3.weight',\n",
       " 'layer3.5.bn3.bias',\n",
       " 'layer4.0.conv1.weight',\n",
       " 'layer4.0.bn1.weight',\n",
       " 'layer4.0.bn1.bias',\n",
       " 'layer4.0.conv2.weight',\n",
       " 'layer4.0.bn2.weight',\n",
       " 'layer4.0.bn2.bias',\n",
       " 'layer4.0.conv3.weight',\n",
       " 'layer4.0.bn3.weight',\n",
       " 'layer4.0.bn3.bias',\n",
       " 'layer4.0.downsample.0.weight',\n",
       " 'layer4.0.downsample.1.weight',\n",
       " 'layer4.0.downsample.1.bias',\n",
       " 'layer4.1.conv1.weight',\n",
       " 'layer4.1.bn1.weight',\n",
       " 'layer4.1.bn1.bias',\n",
       " 'layer4.1.conv2.weight',\n",
       " 'layer4.1.bn2.weight',\n",
       " 'layer4.1.bn2.bias',\n",
       " 'layer4.1.conv3.weight',\n",
       " 'layer4.1.bn3.weight',\n",
       " 'layer4.1.bn3.bias',\n",
       " 'layer4.2.conv1.weight',\n",
       " 'layer4.2.bn1.weight',\n",
       " 'layer4.2.bn1.bias',\n",
       " 'layer4.2.conv2.weight',\n",
       " 'layer4.2.bn2.weight',\n",
       " 'layer4.2.bn2.bias',\n",
       " 'layer4.2.conv3.weight',\n",
       " 'layer4.2.bn3.weight',\n",
       " 'layer4.2.bn3.bias',\n",
       " 'fc.weight',\n",
       " 'fc.bias']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in resnetC.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define phases \n",
    "dloaders = {'trainA':trainsetLoader1, 'trainB':reservedLoaderB, 'trainC':reservedLoader,\n",
    "            'validA':testsetLoader, 'validB':testsetLoader, 'validC':testsetLoader,\n",
    "            'reservedA':reservedLoader, 'reservedB':reservedLoader, 'reservedCA':reservedLoader, 'reservedAB':reservedLoader, 'reservedBA':reservedLoader}\n",
    "model = {'trainA':resnetA, 'trainB':resnetB, 'trainC':resnetC,\n",
    "         'validA':resnetA, 'validB':resnetB, 'validC':resnetC,\n",
    "         'reservedA':resnetA, 'reservedB':resnetB, 'reservedCA':resnetC, 'reservedAB':resnetA, 'reservedBA':resnetB}\n",
    "optimizer = {'trainA':optimizerA, 'trainB':optimizerB, 'trainC':optimizerC,\n",
    "             'validA':optimizerA, 'validB':optimizerB, 'validC':optimizerC,\n",
    "             'reservedA':optimizerA, 'reservedB':optimizerB, 'reservedCA':optimizerC, 'reservedAB':optimizerA, 'reservedBA':optimizerB}\n",
    "criterion = {'trainA':criterionA, 'trainB':criterionA, 'trainC':criterionA,\n",
    "             'validA':criterionA, 'validB':criterionA, 'validC':criterionA,\n",
    "             'reservedA':criterionB, 'reservedB':criterionB, 'reservedCA':criterionB, 'reservedAB':criterionB, 'reservedBA':criterionB}\n",
    "exp_lr_scheduler = {'trainA':exp_lr_schedulerA, 'trainB':exp_lr_schedulerB, 'trainC':exp_lr_schedulerC,\n",
    "             'validA':exp_lr_schedulerA, 'validB':exp_lr_schedulerB, 'validC':exp_lr_schedulerC,\n",
    "             'reservedA':exp_lr_schedulerA, 'reservedB':exp_lr_schedulerB, 'reservedCA':exp_lr_schedulerC, 'reservedAB':exp_lr_schedulerA, 'reservedBA':exp_lr_schedulerB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/59] phase: validA train loss: 0.0017 acc: 0.9227 valid loss: 0.0679 acc: 0.2762\n",
      "\n",
      "Epoch [0/59] phase: validB train loss: 0.0037 acc: 0.8592 valid loss: 0.0086 acc: 0.6297\n",
      "\n",
      "Epoch [0/59] phase: validC train loss: 0.0034 acc: 0.8693 valid loss: 0.0110 acc: 0.4922\n",
      "\n",
      "Epoch [0/59] phase: validC train loss: -0.0022 acc: 0.9177 valid loss: 0.0108 acc: 0.5149\n",
      "\n",
      "Epoch [1/59] phase: validA train loss: 0.0007 acc: 0.9711 valid loss: 0.0630 acc: 0.2862\n",
      "\n",
      "Epoch [1/59] phase: validB train loss: 0.0021 acc: 0.9137 valid loss: 0.0083 acc: 0.6538\n",
      "\n",
      "Epoch [1/59] phase: validC train loss: 0.0018 acc: 0.9242 valid loss: 0.0099 acc: 0.5903\n",
      "\n",
      "Epoch [1/59] phase: validC train loss: -0.0025 acc: 0.9339 valid loss: 0.0083 acc: 0.6481\n",
      "\n",
      "Epoch [2/59] phase: validA train loss: 0.0006 acc: 0.9729 valid loss: 0.0688 acc: 0.2901\n",
      "\n",
      "Epoch [2/59] phase: validB train loss: 0.0018 acc: 0.9240 valid loss: 0.0071 acc: 0.6747\n",
      "\n",
      "Epoch [2/59] phase: validC train loss: 0.0015 acc: 0.9362 valid loss: 0.0086 acc: 0.6211\n",
      "\n",
      "Epoch [2/59] phase: validC train loss: -0.0026 acc: 0.9428 valid loss: 0.0105 acc: 0.5823\n",
      "\n",
      "Epoch [3/59] phase: validA train loss: 0.0005 acc: 0.9757 valid loss: 0.0663 acc: 0.2893\n",
      "\n",
      "Epoch [3/59] phase: validB train loss: 0.0016 acc: 0.9294 valid loss: 0.0080 acc: 0.6455\n",
      "\n",
      "Epoch [3/59] phase: validC train loss: 0.0013 acc: 0.9432 valid loss: 0.0090 acc: 0.6165\n",
      "\n",
      "Epoch [3/59] phase: validC train loss: -0.0025 acc: 0.9490 valid loss: 0.0097 acc: 0.6138\n",
      "\n",
      "Epoch [4/59] phase: validA train loss: 0.0005 acc: 0.9774 valid loss: 0.0678 acc: 0.2818\n",
      "\n",
      "Epoch [4/59] phase: validB train loss: 0.0015 acc: 0.9360 valid loss: 0.0094 acc: 0.6099\n",
      "\n",
      "Epoch [4/59] phase: validC train loss: 0.0012 acc: 0.9478 valid loss: 0.0074 acc: 0.6650\n",
      "\n",
      "Epoch [4/59] phase: validC train loss: -0.0025 acc: 0.9522 valid loss: 0.0078 acc: 0.6659\n",
      "\n",
      "Epoch [5/59] phase: validA train loss: 0.0005 acc: 0.9793 valid loss: 0.0707 acc: 0.2879\n",
      "\n",
      "Epoch [5/59] phase: validB train loss: 0.0014 acc: 0.9370 valid loss: 0.0067 acc: 0.7061\n",
      "\n",
      "Epoch [5/59] phase: validC train loss: 0.0011 acc: 0.9514 valid loss: 0.0104 acc: 0.5994\n",
      "\n",
      "Epoch [5/59] phase: validC train loss: -0.0024 acc: 0.9552 valid loss: 0.0075 acc: 0.6735\n",
      "\n",
      "Epoch [6/59] phase: validA train loss: 0.0004 acc: 0.9802 valid loss: 0.0656 acc: 0.2901\n",
      "\n",
      "Epoch [6/59] phase: validB train loss: 0.0013 acc: 0.9413 valid loss: 0.0094 acc: 0.6439\n",
      "\n",
      "Epoch [6/59] phase: validC train loss: 0.0010 acc: 0.9539 valid loss: 0.0097 acc: 0.6177\n",
      "\n",
      "Epoch [6/59] phase: validC train loss: -0.0024 acc: 0.9575 valid loss: 0.0086 acc: 0.6521\n",
      "\n",
      "Epoch [7/59] phase: validA train loss: 0.0004 acc: 0.9826 valid loss: 0.0705 acc: 0.2889\n",
      "\n",
      "Epoch [7/59] phase: validB train loss: 0.0012 acc: 0.9432 valid loss: 0.0092 acc: 0.6405\n",
      "\n",
      "Epoch [7/59] phase: validC train loss: 0.0009 acc: 0.9564 valid loss: 0.0059 acc: 0.7360\n",
      "\n",
      "Epoch [7/59] phase: validC train loss: -0.0025 acc: 0.9598 valid loss: 0.0073 acc: 0.7073\n",
      "\n",
      "Epoch [8/59] phase: validA train loss: 0.0004 acc: 0.9831 valid loss: 0.0725 acc: 0.2904\n",
      "\n",
      "Epoch [8/59] phase: validB train loss: 0.0012 acc: 0.9463 valid loss: 0.0067 acc: 0.7206\n",
      "\n",
      "Epoch [8/59] phase: validC train loss: 0.0009 acc: 0.9581 valid loss: 0.0080 acc: 0.6728\n",
      "\n",
      "Epoch [8/59] phase: validC train loss: -0.0025 acc: 0.9619 valid loss: 0.0073 acc: 0.7111\n",
      "\n",
      "Epoch [9/59] phase: validA train loss: 0.0004 acc: 0.9828 valid loss: 0.0695 acc: 0.2886\n",
      "\n",
      "Epoch [9/59] phase: validB train loss: 0.0011 acc: 0.9492 valid loss: 0.0067 acc: 0.7295\n",
      "\n",
      "Epoch [9/59] phase: validC train loss: 0.0009 acc: 0.9599 valid loss: 0.0067 acc: 0.7230\n",
      "\n",
      "Epoch [9/59] phase: validC train loss: -0.0025 acc: 0.9642 valid loss: 0.0063 acc: 0.7522\n",
      "\n",
      "Epoch [10/59] phase: validA train loss: 0.0003 acc: 0.9846 valid loss: 0.0697 acc: 0.2907\n",
      "\n",
      "Epoch [10/59] phase: validB train loss: 0.0011 acc: 0.9503 valid loss: 0.0070 acc: 0.7353\n",
      "\n",
      "Epoch [10/59] phase: validC train loss: 0.0008 acc: 0.9621 valid loss: 0.0079 acc: 0.6964\n",
      "\n",
      "Epoch [10/59] phase: validC train loss: -0.0025 acc: 0.9663 valid loss: 0.0082 acc: 0.6993\n",
      "\n",
      "Epoch [11/59] phase: validA train loss: 0.0003 acc: 0.9847 valid loss: 0.0769 acc: 0.2908\n",
      "\n",
      "Epoch [11/59] phase: validB train loss: 0.0011 acc: 0.9503 valid loss: 0.0078 acc: 0.7089\n",
      "\n",
      "Epoch [11/59] phase: validC train loss: 0.0008 acc: 0.9644 valid loss: 0.0068 acc: 0.7325\n",
      "\n",
      "Epoch [11/59] phase: validC train loss: -0.0022 acc: 0.9693 valid loss: 0.0076 acc: 0.7127\n",
      "\n",
      "Epoch [12/59] phase: validA train loss: 0.0003 acc: 0.9853 valid loss: 0.0761 acc: 0.2909\n",
      "\n",
      "Epoch [12/59] phase: validB train loss: 0.0010 acc: 0.9553 valid loss: 0.0074 acc: 0.7059\n",
      "\n",
      "Epoch [12/59] phase: validC train loss: 0.0007 acc: 0.9649 valid loss: 0.0070 acc: 0.7279\n",
      "\n",
      "Epoch [12/59] phase: validC train loss: -0.0023 acc: 0.9708 valid loss: 0.0065 acc: 0.7700\n",
      "\n",
      "Epoch [13/59] phase: validA train loss: 0.0003 acc: 0.9845 valid loss: 0.0706 acc: 0.2885\n",
      "\n",
      "Epoch [13/59] phase: validB train loss: 0.0010 acc: 0.9529 valid loss: 0.0077 acc: 0.7165\n",
      "\n",
      "Epoch [13/59] phase: validC train loss: 0.0007 acc: 0.9661 valid loss: 0.0076 acc: 0.7024\n",
      "\n",
      "Epoch [13/59] phase: validC train loss: -0.0026 acc: 0.9705 valid loss: 0.0076 acc: 0.7277\n",
      "\n",
      "Epoch [14/59] phase: validA train loss: 0.0003 acc: 0.9868 valid loss: 0.0698 acc: 0.2904\n",
      "\n",
      "Epoch [14/59] phase: validB train loss: 0.0009 acc: 0.9562 valid loss: 0.0071 acc: 0.7225\n",
      "\n",
      "Epoch [14/59] phase: validC train loss: 0.0007 acc: 0.9680 valid loss: 0.0079 acc: 0.7091\n",
      "\n",
      "Epoch [14/59] phase: validC train loss: -0.0026 acc: 0.9728 valid loss: 0.0070 acc: 0.7578\n",
      "\n",
      "Epoch [15/59] phase: validA train loss: 0.0003 acc: 0.9856 valid loss: 0.0822 acc: 0.2897\n",
      "\n",
      "Epoch [15/59] phase: validB train loss: 0.0009 acc: 0.9566 valid loss: 0.0083 acc: 0.7067\n",
      "\n",
      "Epoch [15/59] phase: validC train loss: 0.0006 acc: 0.9706 valid loss: 0.0073 acc: 0.7203\n",
      "\n",
      "Epoch [15/59] phase: validC train loss: -0.0023 acc: 0.9742 valid loss: 0.0083 acc: 0.7388\n",
      "\n",
      "Epoch [16/59] phase: validA train loss: 0.0003 acc: 0.9866 valid loss: 0.0744 acc: 0.2901\n",
      "\n",
      "Epoch [16/59] phase: validB train loss: 0.0009 acc: 0.9594 valid loss: 0.0075 acc: 0.7292\n",
      "\n",
      "Epoch [16/59] phase: validC train loss: 0.0006 acc: 0.9706 valid loss: 0.0091 acc: 0.6998\n",
      "\n",
      "Epoch [16/59] phase: validC train loss: -0.0024 acc: 0.9749 valid loss: 0.0070 acc: 0.7555\n",
      "\n",
      "Epoch [17/59] phase: validA train loss: 0.0003 acc: 0.9872 valid loss: 0.0747 acc: 0.2915\n",
      "\n",
      "Epoch [17/59] phase: validB train loss: 0.0009 acc: 0.9595 valid loss: 0.0081 acc: 0.7149\n",
      "\n",
      "Epoch [17/59] phase: validC train loss: 0.0006 acc: 0.9715 valid loss: 0.0104 acc: 0.6756\n",
      "\n",
      "Epoch [17/59] phase: validC train loss: -0.0025 acc: 0.9754 valid loss: 0.0078 acc: 0.7525\n",
      "\n",
      "Epoch [18/59] phase: validA train loss: 0.0002 acc: 0.9887 valid loss: 0.0821 acc: 0.2912\n",
      "\n",
      "Epoch [18/59] phase: validB train loss: 0.0008 acc: 0.9600 valid loss: 0.0077 acc: 0.7362\n",
      "\n",
      "Epoch [18/59] phase: validC train loss: 0.0006 acc: 0.9714 valid loss: 0.0076 acc: 0.7398\n",
      "\n",
      "Epoch [18/59] phase: validC train loss: -0.0023 acc: 0.9776 valid loss: 0.0081 acc: 0.7519\n",
      "\n",
      "Epoch [19/59] phase: validA train loss: 0.0003 acc: 0.9874 valid loss: 0.0795 acc: 0.2908\n",
      "\n",
      "Epoch [19/59] phase: validB train loss: 0.0008 acc: 0.9635 valid loss: 0.0057 acc: 0.7693\n",
      "\n",
      "Epoch [19/59] phase: validC train loss: 0.0006 acc: 0.9738 valid loss: 0.0084 acc: 0.7384\n",
      "\n",
      "Epoch [19/59] phase: validC train loss: -0.0023 acc: 0.9777 valid loss: 0.0080 acc: 0.7497\n",
      "\n",
      "Epoch [20/59] phase: validA train loss: 0.0003 acc: 0.9878 valid loss: 0.0784 acc: 0.2905\n",
      "\n",
      "Epoch [20/59] phase: validB train loss: 0.0008 acc: 0.9627 valid loss: 0.0066 acc: 0.7709\n",
      "\n",
      "Epoch [20/59] phase: validC train loss: 0.0005 acc: 0.9751 valid loss: 0.0071 acc: 0.7723\n",
      "\n",
      "Epoch [20/59] phase: validC train loss: -0.0023 acc: 0.9775 valid loss: 0.0076 acc: 0.7549\n",
      "\n",
      "Epoch [21/59] phase: validA train loss: 0.0002 acc: 0.9895 valid loss: 0.0807 acc: 0.2882\n",
      "\n",
      "Epoch [21/59] phase: validB train loss: 0.0007 acc: 0.9658 valid loss: 0.0066 acc: 0.7752\n",
      "\n",
      "Epoch [21/59] phase: validC train loss: 0.0005 acc: 0.9757 valid loss: 0.0082 acc: 0.7498\n",
      "\n",
      "Epoch [21/59] phase: validC train loss: -0.0023 acc: 0.9797 valid loss: 0.0063 acc: 0.8004\n",
      "\n",
      "Epoch [22/59] phase: validA train loss: 0.0002 acc: 0.9884 valid loss: 0.0726 acc: 0.2900\n",
      "\n",
      "Epoch [22/59] phase: validB train loss: 0.0007 acc: 0.9659 valid loss: 0.0061 acc: 0.7888\n",
      "\n",
      "Epoch [22/59] phase: validC train loss: 0.0005 acc: 0.9778 valid loss: 0.0086 acc: 0.7402\n",
      "\n",
      "Epoch [22/59] phase: validC train loss: -0.0023 acc: 0.9814 valid loss: 0.0083 acc: 0.7745\n",
      "\n",
      "Epoch [23/59] phase: validA train loss: 0.0002 acc: 0.9885 valid loss: 0.0812 acc: 0.2902\n",
      "\n",
      "Epoch [23/59] phase: validB train loss: 0.0007 acc: 0.9655 valid loss: 0.0058 acc: 0.7971\n",
      "\n",
      "Epoch [23/59] phase: validC train loss: 0.0005 acc: 0.9774 valid loss: 0.0102 acc: 0.7099\n",
      "\n",
      "Epoch [23/59] phase: validC train loss: -0.0024 acc: 0.9805 valid loss: 0.0079 acc: 0.7733\n",
      "\n",
      "Epoch [24/59] phase: validA train loss: 0.0002 acc: 0.9896 valid loss: 0.0827 acc: 0.2906\n",
      "\n",
      "Epoch [24/59] phase: validB train loss: 0.0007 acc: 0.9685 valid loss: 0.0059 acc: 0.7925\n",
      "\n",
      "Epoch [24/59] phase: validC train loss: 0.0004 acc: 0.9783 valid loss: 0.0085 acc: 0.7508\n",
      "\n",
      "Epoch [24/59] phase: validC train loss: -0.0024 acc: 0.9826 valid loss: 0.0078 acc: 0.7758\n",
      "\n",
      "Epoch [25/59] phase: validA train loss: 0.0002 acc: 0.9906 valid loss: 0.0810 acc: 0.2906\n",
      "\n",
      "Epoch [25/59] phase: validB train loss: 0.0007 acc: 0.9681 valid loss: 0.0090 acc: 0.7000\n",
      "\n",
      "Epoch [25/59] phase: validC train loss: 0.0004 acc: 0.9806 valid loss: 0.0077 acc: 0.7576\n",
      "\n",
      "Epoch [25/59] phase: validC train loss: -0.0023 acc: 0.9828 valid loss: 0.0086 acc: 0.7551\n",
      "\n",
      "Epoch [26/59] phase: validA train loss: 0.0002 acc: 0.9906 valid loss: 0.0805 acc: 0.2910\n",
      "\n",
      "Epoch [26/59] phase: validB train loss: 0.0006 acc: 0.9702 valid loss: 0.0072 acc: 0.7515\n",
      "\n",
      "Epoch [26/59] phase: validC train loss: 0.0004 acc: 0.9797 valid loss: 0.0084 acc: 0.7398\n",
      "\n",
      "Epoch [26/59] phase: validC train loss: -0.0022 acc: 0.9831 valid loss: 0.0077 acc: 0.7812\n",
      "\n",
      "Epoch [27/59] phase: validA train loss: 0.0002 acc: 0.9911 valid loss: 0.0780 acc: 0.2914\n",
      "\n",
      "Epoch [27/59] phase: validB train loss: 0.0006 acc: 0.9714 valid loss: 0.0078 acc: 0.7573\n",
      "\n",
      "Epoch [27/59] phase: validC train loss: 0.0004 acc: 0.9793 valid loss: 0.0101 acc: 0.7324\n",
      "\n",
      "Epoch [27/59] phase: validC train loss: -0.0024 acc: 0.9847 valid loss: 0.0080 acc: 0.7770\n",
      "\n",
      "Epoch [28/59] phase: validA train loss: 0.0002 acc: 0.9906 valid loss: 0.0762 acc: 0.2896\n",
      "\n",
      "Epoch [28/59] phase: validB train loss: 0.0006 acc: 0.9715 valid loss: 0.0080 acc: 0.7487\n",
      "\n",
      "Epoch [28/59] phase: validC train loss: 0.0004 acc: 0.9812 valid loss: 0.0100 acc: 0.7276\n",
      "\n",
      "Epoch [28/59] phase: validC train loss: -0.0025 acc: 0.9845 valid loss: 0.0085 acc: 0.7652\n",
      "\n",
      "Epoch [29/59] phase: validA train loss: 0.0002 acc: 0.9921 valid loss: 0.0859 acc: 0.2906\n",
      "\n",
      "Epoch [29/59] phase: validB train loss: 0.0006 acc: 0.9731 valid loss: 0.0060 acc: 0.8021\n",
      "\n",
      "Epoch [29/59] phase: validC train loss: 0.0004 acc: 0.9816 valid loss: 0.0080 acc: 0.7796\n",
      "\n",
      "Epoch [29/59] phase: validC train loss: -0.0023 acc: 0.9848 valid loss: 0.0086 acc: 0.7708\n",
      "\n",
      "Epoch [30/59] phase: validA train loss: 0.0002 acc: 0.9908 valid loss: 0.0827 acc: 0.2907\n",
      "\n",
      "Epoch [30/59] phase: validB train loss: 0.0006 acc: 0.9718 valid loss: 0.0074 acc: 0.7732\n",
      "\n",
      "Epoch [30/59] phase: validC train loss: 0.0004 acc: 0.9824 valid loss: 0.0093 acc: 0.7542\n",
      "\n",
      "Epoch [30/59] phase: validC train loss: -0.0023 acc: 0.9861 valid loss: 0.0083 acc: 0.7852\n",
      "\n",
      "Epoch [31/59] phase: validA train loss: 0.0001 acc: 0.9926 valid loss: 0.0878 acc: 0.2908\n",
      "\n",
      "Epoch [31/59] phase: validB train loss: 0.0005 acc: 0.9753 valid loss: 0.0059 acc: 0.8102\n",
      "\n",
      "Epoch [31/59] phase: validC train loss: 0.0003 acc: 0.9840 valid loss: 0.0086 acc: 0.7765\n",
      "\n",
      "Epoch [31/59] phase: validC train loss: -0.0023 acc: 0.9861 valid loss: 0.0081 acc: 0.7691\n",
      "\n",
      "Epoch [32/59] phase: validA train loss: 0.0002 acc: 0.9915 valid loss: 0.0824 acc: 0.2878\n",
      "\n",
      "Epoch [32/59] phase: validB train loss: 0.0005 acc: 0.9748 valid loss: 0.0064 acc: 0.7935\n",
      "\n",
      "Epoch [32/59] phase: validC train loss: 0.0003 acc: 0.9844 valid loss: 0.0085 acc: 0.7574\n",
      "\n",
      "Epoch [32/59] phase: validC train loss: -0.0023 acc: 0.9857 valid loss: 0.0076 acc: 0.7882\n",
      "\n",
      "Epoch [33/59] phase: validA train loss: 0.0002 acc: 0.9925 valid loss: 0.0869 acc: 0.2913\n",
      "\n",
      "Epoch [33/59] phase: validB train loss: 0.0005 acc: 0.9757 valid loss: 0.0059 acc: 0.8087\n",
      "\n",
      "Epoch [33/59] phase: validC train loss: 0.0003 acc: 0.9845 valid loss: 0.0091 acc: 0.7444\n",
      "\n",
      "Epoch [33/59] phase: validC train loss: -0.0023 acc: 0.9873 valid loss: 0.0083 acc: 0.7724\n",
      "\n",
      "Epoch [34/59] phase: validA train loss: 0.0002 acc: 0.9916 valid loss: 0.0832 acc: 0.2907\n",
      "\n",
      "Epoch [34/59] phase: validB train loss: 0.0005 acc: 0.9773 valid loss: 0.0070 acc: 0.7991\n",
      "\n",
      "Epoch [34/59] phase: validC train loss: 0.0003 acc: 0.9855 valid loss: 0.0112 acc: 0.7267\n",
      "\n",
      "Epoch [34/59] phase: validC train loss: -0.0023 acc: 0.9871 valid loss: 0.0087 acc: 0.7772\n",
      "\n",
      "Epoch [35/59] phase: validA train loss: 0.0002 acc: 0.9928 valid loss: 0.0932 acc: 0.2912\n",
      "\n",
      "Epoch [35/59] phase: validB train loss: 0.0005 acc: 0.9788 valid loss: 0.0068 acc: 0.7970\n",
      "\n",
      "Epoch [35/59] phase: validC train loss: 0.0003 acc: 0.9858 valid loss: 0.0102 acc: 0.7410\n",
      "\n",
      "Epoch [35/59] phase: validC train loss: -0.0022 acc: 0.9878 valid loss: 0.0092 acc: 0.7659\n",
      "\n",
      "Epoch [36/59] phase: validA train loss: 0.0001 acc: 0.9925 valid loss: 0.0958 acc: 0.2907\n",
      "\n",
      "Epoch [36/59] phase: validB train loss: 0.0005 acc: 0.9765 valid loss: 0.0071 acc: 0.7979\n",
      "\n",
      "Epoch [36/59] phase: validC train loss: 0.0003 acc: 0.9873 valid loss: 0.0104 acc: 0.7381\n",
      "\n",
      "Epoch [36/59] phase: validC train loss: -0.0022 acc: 0.9888 valid loss: 0.0095 acc: 0.7658\n",
      "\n",
      "Epoch [37/59] phase: validA train loss: 0.0001 acc: 0.9942 valid loss: 0.0998 acc: 0.2912\n",
      "\n",
      "Epoch [37/59] phase: validB train loss: 0.0004 acc: 0.9783 valid loss: 0.0080 acc: 0.7855\n",
      "\n",
      "Epoch [37/59] phase: validC train loss: 0.0003 acc: 0.9850 valid loss: 0.0102 acc: 0.7381\n",
      "\n",
      "Epoch [37/59] phase: validC train loss: -0.0022 acc: 0.9889 valid loss: 0.0083 acc: 0.7839\n",
      "\n",
      "Epoch [38/59] phase: validA train loss: 0.0002 acc: 0.9933 valid loss: 0.0911 acc: 0.2908\n",
      "\n",
      "Epoch [38/59] phase: validB train loss: 0.0004 acc: 0.9796 valid loss: 0.0080 acc: 0.7771\n",
      "\n",
      "Epoch [38/59] phase: validC train loss: 0.0003 acc: 0.9862 valid loss: 0.0085 acc: 0.7824\n",
      "\n",
      "Epoch [38/59] phase: validC train loss: -0.0023 acc: 0.9895 valid loss: 0.0089 acc: 0.7887\n",
      "\n",
      "Epoch [39/59] phase: validA train loss: 0.0001 acc: 0.9945 valid loss: 0.0925 acc: 0.2909\n",
      "\n",
      "Epoch [39/59] phase: validB train loss: 0.0004 acc: 0.9799 valid loss: 0.0068 acc: 0.8090\n",
      "\n",
      "Epoch [39/59] phase: validC train loss: 0.0002 acc: 0.9881 valid loss: 0.0107 acc: 0.7542\n",
      "\n",
      "Epoch [39/59] phase: validC train loss: -0.0023 acc: 0.9907 valid loss: 0.0090 acc: 0.7911\n",
      "\n",
      "Epoch [40/59] phase: validA train loss: 0.0001 acc: 0.9940 valid loss: 0.0852 acc: 0.2899\n",
      "\n",
      "Epoch [40/59] phase: validB train loss: 0.0004 acc: 0.9807 valid loss: 0.0077 acc: 0.7926\n",
      "\n",
      "Epoch [40/59] phase: validC train loss: 0.0003 acc: 0.9880 valid loss: 0.0108 acc: 0.7485\n",
      "\n",
      "Epoch [40/59] phase: validC train loss: -0.0023 acc: 0.9896 valid loss: 0.0095 acc: 0.7803\n",
      "\n",
      "Epoch [41/59] phase: validA train loss: 0.0001 acc: 0.9937 valid loss: 0.0919 acc: 0.2908\n",
      "\n",
      "Epoch [41/59] phase: validB train loss: 0.0004 acc: 0.9804 valid loss: 0.0071 acc: 0.7977\n",
      "\n",
      "Epoch [41/59] phase: validC train loss: 0.0003 acc: 0.9877 valid loss: 0.0098 acc: 0.7681\n",
      "\n",
      "Epoch [41/59] phase: validC train loss: -0.0022 acc: 0.9899 valid loss: 0.0084 acc: 0.7910\n",
      "\n",
      "Epoch [42/59] phase: validA train loss: 0.0001 acc: 0.9930 valid loss: 0.0895 acc: 0.2913\n",
      "\n",
      "Epoch [42/59] phase: validB train loss: 0.0004 acc: 0.9823 valid loss: 0.0082 acc: 0.7978\n",
      "\n",
      "Epoch [42/59] phase: validC train loss: 0.0002 acc: 0.9888 valid loss: 0.0107 acc: 0.7369\n",
      "\n",
      "Epoch [42/59] phase: validC train loss: -0.0022 acc: 0.9914 valid loss: 0.0082 acc: 0.7923\n",
      "\n",
      "Epoch [43/59] phase: validA train loss: 0.0001 acc: 0.9949 valid loss: 0.0904 acc: 0.2916\n",
      "\n",
      "Epoch [43/59] phase: validB train loss: 0.0004 acc: 0.9820 valid loss: 0.0084 acc: 0.7740\n",
      "\n",
      "Epoch [43/59] phase: validC train loss: 0.0002 acc: 0.9889 valid loss: 0.0097 acc: 0.7639\n",
      "\n",
      "Epoch [43/59] phase: validC train loss: -0.0022 acc: 0.9919 valid loss: 0.0088 acc: 0.7873\n",
      "\n",
      "Epoch [44/59] phase: validA train loss: 0.0001 acc: 0.9930 valid loss: 0.0862 acc: 0.2909\n",
      "\n",
      "Epoch [44/59] phase: validB train loss: 0.0003 acc: 0.9830 valid loss: 0.0077 acc: 0.7886\n",
      "\n",
      "Epoch [44/59] phase: validC train loss: 0.0002 acc: 0.9895 valid loss: 0.0106 acc: 0.7608\n",
      "\n",
      "Epoch [44/59] phase: validC train loss: -0.0024 acc: 0.9919 valid loss: 0.0087 acc: 0.7855\n",
      "\n",
      "Epoch [45/59] phase: validA train loss: 0.0001 acc: 0.9930 valid loss: 0.0874 acc: 0.2909\n",
      "\n",
      "Epoch [45/59] phase: validB train loss: 0.0004 acc: 0.9837 valid loss: 0.0084 acc: 0.7743\n",
      "\n",
      "Epoch [45/59] phase: validC train loss: 0.0002 acc: 0.9891 valid loss: 0.0105 acc: 0.7431\n",
      "\n",
      "Epoch [45/59] phase: validC train loss: -0.0022 acc: 0.9917 valid loss: 0.0093 acc: 0.7762\n",
      "\n",
      "Epoch [46/59] phase: validA train loss: 0.0001 acc: 0.9944 valid loss: 0.0900 acc: 0.2911\n",
      "\n",
      "Epoch [46/59] phase: validB train loss: 0.0003 acc: 0.9839 valid loss: 0.0085 acc: 0.7919\n",
      "\n",
      "Epoch [46/59] phase: validC train loss: 0.0002 acc: 0.9900 valid loss: 0.0107 acc: 0.7545\n",
      "\n",
      "Epoch [46/59] phase: validC train loss: -0.0022 acc: 0.9926 valid loss: 0.0091 acc: 0.7822\n",
      "\n",
      "Epoch [47/59] phase: validA train loss: 0.0001 acc: 0.9946 valid loss: 0.0931 acc: 0.2913\n",
      "\n",
      "Epoch [47/59] phase: validB train loss: 0.0003 acc: 0.9833 valid loss: 0.0071 acc: 0.8138\n",
      "\n",
      "Epoch [47/59] phase: validC train loss: 0.0002 acc: 0.9914 valid loss: 0.0094 acc: 0.7691\n",
      "\n",
      "Epoch [47/59] phase: validC train loss: -0.0022 acc: 0.9925 valid loss: 0.0106 acc: 0.7641\n",
      "\n",
      "Epoch [48/59] phase: validA train loss: 0.0001 acc: 0.9948 valid loss: 0.0936 acc: 0.2878\n",
      "\n",
      "Epoch [48/59] phase: validB train loss: 0.0003 acc: 0.9843 valid loss: 0.0078 acc: 0.8052\n",
      "\n",
      "Epoch [48/59] phase: validC train loss: 0.0002 acc: 0.9889 valid loss: 0.0104 acc: 0.7466\n",
      "\n",
      "Epoch [48/59] phase: validC train loss: -0.0021 acc: 0.9922 valid loss: 0.0096 acc: 0.7690\n",
      "\n",
      "Epoch [49/59] phase: validA train loss: 0.0001 acc: 0.9958 valid loss: 0.1000 acc: 0.2898\n",
      "\n",
      "Epoch [49/59] phase: validB train loss: 0.0003 acc: 0.9843 valid loss: 0.0077 acc: 0.7882\n",
      "\n",
      "Epoch [49/59] phase: validC train loss: 0.0002 acc: 0.9917 valid loss: 0.0097 acc: 0.7660\n",
      "\n",
      "Epoch [49/59] phase: validC train loss: -0.0022 acc: 0.9940 valid loss: 0.0094 acc: 0.7835\n",
      "\n",
      "Epoch [50/59] phase: validA train loss: 0.0001 acc: 0.9953 valid loss: 0.1123 acc: 0.2896\n",
      "\n",
      "Epoch [50/59] phase: validB train loss: 0.0003 acc: 0.9865 valid loss: 0.0087 acc: 0.7880\n",
      "\n",
      "Epoch [50/59] phase: validC train loss: 0.0002 acc: 0.9915 valid loss: 0.0118 acc: 0.7459\n",
      "\n",
      "Epoch [50/59] phase: validC train loss: -0.0021 acc: 0.9920 valid loss: 0.0111 acc: 0.7582\n",
      "\n",
      "Epoch [51/59] phase: validA train loss: 0.0001 acc: 0.9949 valid loss: 0.0954 acc: 0.2913\n",
      "\n",
      "Epoch [51/59] phase: validB train loss: 0.0003 acc: 0.9860 valid loss: 0.0081 acc: 0.7913\n",
      "\n",
      "Epoch [51/59] phase: validC train loss: 0.0002 acc: 0.9917 valid loss: 0.0100 acc: 0.7591\n",
      "\n",
      "Epoch [51/59] phase: validC train loss: -0.0021 acc: 0.9933 valid loss: 0.0101 acc: 0.7770\n",
      "\n",
      "Epoch [52/59] phase: validA train loss: 0.0001 acc: 0.9955 valid loss: 0.1014 acc: 0.2903\n",
      "\n",
      "Epoch [52/59] phase: validB train loss: 0.0003 acc: 0.9878 valid loss: 0.0081 acc: 0.7965\n",
      "\n",
      "Epoch [52/59] phase: validC train loss: 0.0002 acc: 0.9924 valid loss: 0.0125 acc: 0.7364\n",
      "\n",
      "Epoch [52/59] phase: validC train loss: -0.0022 acc: 0.9937 valid loss: 0.0104 acc: 0.7752\n",
      "\n",
      "Epoch [53/59] phase: validA train loss: 0.0001 acc: 0.9949 valid loss: 0.0935 acc: 0.2909\n",
      "\n",
      "Epoch [53/59] phase: validB train loss: 0.0003 acc: 0.9863 valid loss: 0.0084 acc: 0.7891\n",
      "\n",
      "Epoch [53/59] phase: validC train loss: 0.0001 acc: 0.9933 valid loss: 0.0096 acc: 0.7772\n",
      "\n",
      "Epoch [53/59] phase: validC train loss: -0.0022 acc: 0.9930 valid loss: 0.0104 acc: 0.7755\n",
      "\n",
      "Epoch [54/59] phase: validA train loss: 0.0001 acc: 0.9953 valid loss: 0.0929 acc: 0.2908\n",
      "\n",
      "Epoch [54/59] phase: validB train loss: 0.0003 acc: 0.9856 valid loss: 0.0101 acc: 0.7557\n",
      "\n",
      "Epoch [54/59] phase: validC train loss: 0.0001 acc: 0.9935 valid loss: 0.0107 acc: 0.7725\n",
      "\n",
      "Epoch [54/59] phase: validC train loss: -0.0022 acc: 0.9935 valid loss: 0.0100 acc: 0.7814\n",
      "\n",
      "Epoch [55/59] phase: validA train loss: 0.0001 acc: 0.9956 valid loss: 0.0920 acc: 0.2907\n",
      "\n",
      "Epoch [55/59] phase: validB train loss: 0.0004 acc: 0.9810 valid loss: 0.0074 acc: 0.7954\n",
      "\n",
      "Epoch [55/59] phase: validC train loss: 0.0002 acc: 0.9923 valid loss: 0.0102 acc: 0.7667\n",
      "\n",
      "Epoch [55/59] phase: validC train loss: -0.0023 acc: 0.9938 valid loss: 0.0106 acc: 0.7702\n",
      "\n",
      "Epoch [56/59] phase: validA train loss: 0.0001 acc: 0.9964 valid loss: 0.0906 acc: 0.2920\n",
      "\n",
      "Epoch [56/59] phase: validB train loss: 0.0003 acc: 0.9857 valid loss: 0.0073 acc: 0.8037\n",
      "\n",
      "Epoch [56/59] phase: validC train loss: 0.0002 acc: 0.9919 valid loss: 0.0124 acc: 0.7356\n",
      "\n",
      "Epoch [56/59] phase: validC train loss: -0.0023 acc: 0.9938 valid loss: 0.0104 acc: 0.7744\n",
      "\n",
      "Epoch [57/59] phase: validA train loss: 0.0001 acc: 0.9960 valid loss: 0.0921 acc: 0.2914\n",
      "\n",
      "Epoch [57/59] phase: validB train loss: 0.0003 acc: 0.9864 valid loss: 0.0077 acc: 0.8028\n",
      "\n",
      "Epoch [57/59] phase: validC train loss: 0.0002 acc: 0.9923 valid loss: 0.0120 acc: 0.7552\n",
      "\n",
      "Epoch [57/59] phase: validC train loss: -0.0022 acc: 0.9933 valid loss: 0.0093 acc: 0.7867\n",
      "\n",
      "Epoch [58/59] phase: validA train loss: 0.0001 acc: 0.9958 valid loss: 0.0942 acc: 0.2901\n",
      "\n",
      "Epoch [58/59] phase: validB train loss: 0.0002 acc: 0.9885 valid loss: 0.0079 acc: 0.8012\n",
      "\n",
      "Epoch [58/59] phase: validC train loss: 0.0002 acc: 0.9925 valid loss: 0.0110 acc: 0.7528\n",
      "\n",
      "Epoch [58/59] phase: validC train loss: -0.0021 acc: 0.9931 valid loss: 0.0099 acc: 0.7857\n",
      "\n",
      "Epoch [59/59] phase: validA train loss: 0.0001 acc: 0.9960 valid loss: 0.0899 acc: 0.2910\n",
      "\n",
      "Epoch [59/59] phase: validB train loss: 0.0003 acc: 0.9873 valid loss: 0.0079 acc: 0.8006\n",
      "\n",
      "Epoch [59/59] phase: validC train loss: 0.0002 acc: 0.9935 valid loss: 0.0120 acc: 0.7419\n",
      "\n",
      "Epoch [59/59] phase: validC train loss: -0.0022 acc: 0.9948 valid loss: 0.0105 acc: 0.7682\n",
      "\n",
      "Best val Acc: 0.813800\n",
      "Training time: 170.342415 minutes\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"#### bn acti conv and fc - unlimited bn - adam learning rate 0.001 - scheduler 20 - opt adamw ####\")\n",
    "\n",
    "start_time = time.time()\n",
    "model = train_model(dloaders, model, criterion, optimizer, exp_lr_scheduler, num_epochs=60)\n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5149,\n",
       " 0.6481,\n",
       " 0.5823,\n",
       " 0.6138,\n",
       " 0.6659,\n",
       " 0.6735,\n",
       " 0.6521,\n",
       " 0.7073,\n",
       " 0.7111000000000001,\n",
       " 0.7522000000000001,\n",
       " 0.6993,\n",
       " 0.7127,\n",
       " 0.77,\n",
       " 0.7277,\n",
       " 0.7578,\n",
       " 0.7388,\n",
       " 0.7555000000000001,\n",
       " 0.7525000000000001,\n",
       " 0.7519,\n",
       " 0.7497,\n",
       " 0.7549,\n",
       " 0.8004,\n",
       " 0.7745000000000001,\n",
       " 0.7733,\n",
       " 0.7758,\n",
       " 0.7551,\n",
       " 0.7812,\n",
       " 0.777,\n",
       " 0.7652,\n",
       " 0.7708,\n",
       " 0.7852,\n",
       " 0.7691,\n",
       " 0.7882,\n",
       " 0.7724000000000001,\n",
       " 0.7772,\n",
       " 0.7659,\n",
       " 0.7658,\n",
       " 0.7839,\n",
       " 0.7887000000000001,\n",
       " 0.7911,\n",
       " 0.7803,\n",
       " 0.791,\n",
       " 0.7923,\n",
       " 0.7873,\n",
       " 0.7855000000000001,\n",
       " 0.7762,\n",
       " 0.7822,\n",
       " 0.7641,\n",
       " 0.769,\n",
       " 0.7835000000000001,\n",
       " 0.7582,\n",
       " 0.777,\n",
       " 0.7752,\n",
       " 0.7755000000000001,\n",
       " 0.7814,\n",
       " 0.7702,\n",
       " 0.7744000000000001,\n",
       " 0.7867000000000001,\n",
       " 0.7857000000000001,\n",
       " 0.7682]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[443.,   0.,   4.,   6.,   2.,   2., 512.,   0.,  29.,   2.],\n",
      "        [  1., 940.,   0.,   8.,   3.,   0.,  38.,   0.,   4.,   6.],\n",
      "        [  2.,   0., 264.,   3., 122.,   0., 592.,   0.,  17.,   0.],\n",
      "        [ 10.,   7.,   6., 686.,  19.,   0., 243.,   0.,  16.,  13.],\n",
      "        [  1.,   0.,  12.,  17., 611.,   0., 344.,   0.,  14.,   1.],\n",
      "        [  0.,   0.,   1.,   0.,   0., 913.,   2.,  35.,  11.,  38.],\n",
      "        [ 12.,   0.,   6.,   8.,  20.,   1., 933.,   0.,  18.,   2.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,  11.,   0., 918.,   3.,  68.],\n",
      "        [  0.,   0.,   0.,   1.,   0.,   1.,   7.,   0., 991.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   1.,   0.,  15.,   1., 983.]])\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = torch.zeros(10, 10)\n",
    "for inputs, labels in dloaders['validC']:\n",
    "    inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n",
    "\n",
    "    outputs = model['validC'](inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    \n",
    "    for t, p in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
