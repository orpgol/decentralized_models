{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['','--num_channels','3','--model','resnet','--epochs','75',\n",
    "            '--gpu','2','--num_users','3','--dataset', 'cifar', '--lr','0.1', \n",
    "            '--local_ep','1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(bs=128, dataset='cifar', epochs=75, frac=0.1, gpu=2, iid=False, kernel_num=9, kernel_sizes='3,4,5', local_bs=10, local_ep=1, lr=0.1, max_pool='True', model='resnet', momentum=0.5, norm='batch_norm', num_channels=3, num_classes=10, num_filters=32, num_users=3, seed=1, split='user', stopping_rounds=10, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classDict = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "\n",
    "# Define a function to separate CIFAR classes by class index\n",
    "\n",
    "def get_class_i(x, y, i):\n",
    "    \"\"\"\n",
    "    x: trainset.train_data or testset.test_data\n",
    "    y: trainset.train_labels or testset.test_labels\n",
    "    i: class label, a number between 0 to 9\n",
    "    return: x_i\n",
    "    \"\"\"\n",
    "    # Convert to a numpy array\n",
    "    y = np.array(y)\n",
    "    # Locate position of labels that equal to i\n",
    "    pos_i = np.argwhere(y == i)\n",
    "    # Convert the result into a 1-D list\n",
    "    pos_i = list(pos_i[:,0])\n",
    "    # Collect all data that match the desired label\n",
    "#     x_i = [x[j] for j in pos_i]\n",
    "    \n",
    "    return pos_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(dataset_train):\n",
    "    \n",
    "    frac = int(len(dataset_train.data) * 0.05)\n",
    "    x_reserve = dataset_train.data[:frac]\n",
    "    y_reserve = dataset_train.targets[:frac]\n",
    "    x_train = dataset_train.data[frac:]\n",
    "    y_train = dataset_train.targets[frac:]\n",
    "    \n",
    "    reserved = get_class_i(x_reserve, y_reserve, classDict['plane']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['car']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['bird']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['cat']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['deer']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['dog']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['frog']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['horse']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['ship']) \\\n",
    "               +get_class_i(x_reserve, y_reserve, classDict['truck'])\n",
    "\n",
    "    train1 = get_class_i(x_train, y_train, classDict['plane']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['car']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['bird'])\n",
    "\n",
    "    train2 = get_class_i(x_train, y_train, classDict['cat']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['deer']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['dog'])\n",
    "    \n",
    "    train3 = get_class_i(x_train, y_train, classDict['frog']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['horse']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['ship']) \\\n",
    "             +get_class_i(x_train, y_train, classDict['truck'])\n",
    "    \n",
    "    dict_users = {0: set(reserved+train3), 1:set(train1), 2:set(train2)}\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n",
      "Round   0, Average loss 2.906\n",
      "Testing accuracy: 10.00\n",
      "Round   1, Average loss 2.147\n",
      "Testing accuracy: 10.00\n",
      "Round   2, Average loss 2.024\n",
      "Testing accuracy: 11.00\n",
      "Round   3, Average loss 1.916\n",
      "Testing accuracy: 26.00\n",
      "Round   4, Average loss 1.799\n",
      "Testing accuracy: 39.00\n",
      "Round   5, Average loss 1.751\n",
      "Testing accuracy: 40.00\n",
      "Round   6, Average loss 1.678\n",
      "Testing accuracy: 47.00\n",
      "Round   7, Average loss 1.598\n",
      "Testing accuracy: 49.00\n",
      "Round   8, Average loss 1.539\n",
      "Testing accuracy: 52.00\n",
      "Round   9, Average loss 1.500\n",
      "Testing accuracy: 50.00\n",
      "Round  10, Average loss 1.464\n",
      "Testing accuracy: 53.00\n",
      "Round  11, Average loss 1.416\n",
      "Testing accuracy: 57.00\n",
      "Round  12, Average loss 1.354\n",
      "Testing accuracy: 59.00\n",
      "Round  13, Average loss 1.348\n",
      "Testing accuracy: 59.00\n",
      "Round  14, Average loss 1.293\n",
      "Testing accuracy: 60.00\n",
      "Round  15, Average loss 1.253\n",
      "Testing accuracy: 63.00\n",
      "Round  16, Average loss 1.228\n",
      "Testing accuracy: 50.00\n",
      "Round  17, Average loss 1.213\n",
      "Testing accuracy: 64.00\n",
      "Round  18, Average loss 1.165\n",
      "Testing accuracy: 48.00\n",
      "Round  19, Average loss 1.123\n",
      "Testing accuracy: 66.00\n",
      "Round  20, Average loss 1.082\n",
      "Testing accuracy: 68.00\n",
      "Round  21, Average loss 1.024\n",
      "Testing accuracy: 67.00\n",
      "Round  22, Average loss 1.036\n",
      "Testing accuracy: 66.00\n",
      "Round  23, Average loss 0.972\n",
      "Testing accuracy: 70.00\n",
      "Round  24, Average loss 0.940\n",
      "Testing accuracy: 71.00\n",
      "Round  25, Average loss 0.910\n",
      "Testing accuracy: 71.00\n",
      "Round  26, Average loss 0.877\n",
      "Testing accuracy: 72.00\n",
      "Round  27, Average loss 0.877\n",
      "Testing accuracy: 71.00\n",
      "Round  28, Average loss 0.840\n",
      "Testing accuracy: 72.00\n",
      "Round  29, Average loss 0.804\n",
      "Testing accuracy: 72.00\n",
      "Round  30, Average loss 0.825\n",
      "Testing accuracy: 71.00\n",
      "Round  31, Average loss 0.766\n",
      "Testing accuracy: 74.00\n",
      "Round  32, Average loss 0.741\n",
      "Testing accuracy: 73.00\n",
      "Round  33, Average loss 0.724\n",
      "Testing accuracy: 74.00\n",
      "Round  34, Average loss 0.704\n",
      "Testing accuracy: 74.00\n",
      "Round  35, Average loss 0.709\n",
      "Testing accuracy: 74.00\n",
      "Round  36, Average loss 0.662\n",
      "Testing accuracy: 75.00\n",
      "Round  37, Average loss 0.640\n",
      "Testing accuracy: 75.00\n",
      "Round  38, Average loss 0.624\n",
      "Testing accuracy: 76.00\n",
      "Round  39, Average loss 0.610\n",
      "Testing accuracy: 75.00\n",
      "Round  40, Average loss 0.597\n",
      "Testing accuracy: 75.00\n",
      "Round  41, Average loss 0.569\n",
      "Testing accuracy: 75.00\n",
      "Round  42, Average loss 0.547\n",
      "Testing accuracy: 74.00\n",
      "Round  43, Average loss 0.546\n",
      "Testing accuracy: 76.00\n",
      "Round  44, Average loss 0.515\n",
      "Testing accuracy: 77.00\n",
      "Round  45, Average loss 0.487\n",
      "Testing accuracy: 75.00\n",
      "Round  46, Average loss 0.491\n",
      "Testing accuracy: 76.00\n",
      "Round  47, Average loss 0.458\n",
      "Testing accuracy: 77.00\n",
      "Round  48, Average loss 0.450\n",
      "Testing accuracy: 77.00\n",
      "Round  49, Average loss 0.436\n",
      "Testing accuracy: 77.00\n",
      "Round  50, Average loss 0.411\n",
      "Testing accuracy: 76.00\n",
      "Round  51, Average loss 0.387\n",
      "Testing accuracy: 77.00\n",
      "Round  52, Average loss 0.366\n",
      "Testing accuracy: 77.00\n",
      "Round  53, Average loss 0.490\n",
      "Testing accuracy: 77.00\n",
      "Round  54, Average loss 0.358\n",
      "Testing accuracy: 77.00\n",
      "Round  55, Average loss 0.320\n",
      "Testing accuracy: 77.00\n",
      "Round  56, Average loss 0.323\n",
      "Testing accuracy: 77.00\n",
      "Round  57, Average loss 0.308\n",
      "Testing accuracy: 77.00\n",
      "Round  58, Average loss 0.314\n",
      "Testing accuracy: 77.00\n",
      "Round  59, Average loss 0.307\n",
      "Testing accuracy: 77.00\n",
      "Round  60, Average loss 0.264\n",
      "Testing accuracy: 77.00\n",
      "Round  61, Average loss 0.268\n",
      "Testing accuracy: 77.00\n",
      "Round  62, Average loss 0.244\n",
      "Testing accuracy: 77.00\n",
      "Round  63, Average loss 0.220\n",
      "Testing accuracy: 77.00\n",
      "Round  64, Average loss 0.212\n",
      "Testing accuracy: 76.00\n",
      "Round  65, Average loss 0.210\n",
      "Testing accuracy: 77.00\n",
      "Round  66, Average loss 0.192\n",
      "Testing accuracy: 77.00\n",
      "Round  67, Average loss 0.188\n",
      "Testing accuracy: 77.00\n",
      "Round  68, Average loss 0.171\n",
      "Testing accuracy: 77.00\n",
      "Round  69, Average loss 0.166\n",
      "Testing accuracy: 77.00\n",
      "Round  70, Average loss 0.161\n",
      "Testing accuracy: 78.00\n",
      "Round  71, Average loss 0.145\n",
      "Testing accuracy: 77.00\n",
      "Round  72, Average loss 0.139\n",
      "Testing accuracy: 77.00\n",
      "Round  73, Average loss 0.144\n",
      "Testing accuracy: 78.00\n",
      "Round  74, Average loss 0.129\n",
      "Testing accuracy: 77.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    writer = SummaryWriter('../../runs/') \n",
    "    \n",
    "    # parse args\n",
    "    args = args_parser()\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "    # load dataset and split users\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "        # sample users\n",
    "        if args.iid:\n",
    "            dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "        if args.iid:\n",
    "            dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            dict_users = create_dict(dataset_train)\n",
    "    else:\n",
    "        exit('Error: unrecognized dataset')\n",
    "    img_size = dataset_train[0][0].shape\n",
    "\n",
    "    # build model\n",
    "    if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "        net_glob = CNNCifar(args=args).to(args.device)\n",
    "    elif args.model == 'resnet' and args.dataset == 'cifar':\n",
    "        net_glob = models.resnet50(pretrained=True)\n",
    "        num_ftrs = net_glob.fc.in_features\n",
    "        net_glob.fc = torch.nn.Linear(num_ftrs, 10)\n",
    "        net_glob.to(args.device)\n",
    "    elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "        net_glob = CNNMnist(args=args).to(args.device)\n",
    "    elif args.model == 'mlp':\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "    print(net_glob)\n",
    "    net_glob.train()\n",
    "\n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "\n",
    "    # training\n",
    "    loss_train = []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    net_best = None\n",
    "    best_loss = None\n",
    "    val_acc_list, net_list = [], []\n",
    "\n",
    "    for iter in range(args.epochs):\n",
    "        w_locals, loss_locals = [], []\n",
    "        m = max(int(args.frac * args.num_users), 3)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        for idx in idxs_users:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "            w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "        # update global weights\n",
    "        w_glob = FedAvg(w_locals)\n",
    "\n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "        loss_train.append(loss_avg)\n",
    "        \n",
    "        # testing\n",
    "        net_glob.eval()\n",
    "        acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "        print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "        writer.add_scalar('train/loss_federated2', loss_avg, iter)\n",
    "        writer.add_scalar('valid/accuracy_federated2', acc_test.data.numpy()/100., iter)\n",
    "        \n",
    "        net_glob.train()\n",
    "\n",
    "    # plot loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_train)), loss_train)\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "confusion_matrix = torch.zeros(10, 10)\n",
    "net_glob.eval()\n",
    "# testing\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "data_loader = DataLoader(dataset_test, batch_size=args.bs)\n",
    "l = len(data_loader)\n",
    "for idx, (data, target) in enumerate(data_loader):\n",
    "    if args.gpu != -1:\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "    log_probs = net_glob(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "    # get the index of the max log-probability\n",
    "    y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "    correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n",
    "\n",
    "    for t, p in zip(target.data, y_pred.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[817.,   8.,  40.,  13.,  15.,   3.,  10.,  12.,  59.,  23.],\n",
      "        [ 17., 873.,   6.,   8.,   1.,   5.,   6.,   3.,  32.,  49.],\n",
      "        [ 52.,   3., 699.,  42.,  67.,  58.,  36.,  26.,  10.,   7.],\n",
      "        [ 18.,   6.,  61., 536.,  71., 180.,  38.,  63.,  16.,  11.],\n",
      "        [ 11.,   1.,  49.,  50., 774.,  32.,  21.,  52.,  10.,   0.],\n",
      "        [  8.,   2.,  35., 109.,  55., 705.,  14.,  64.,   6.,   2.],\n",
      "        [  5.,   5.,  39.,  57.,  29.,  31., 808.,  12.,  11.,   3.],\n",
      "        [ 13.,   0.,  20.,  24.,  43.,  37.,   5., 848.,   4.,   6.],\n",
      "        [ 54.,  20.,  11.,   6.,   9.,   6.,   5.,   6., 870.,  13.],\n",
      "        [ 32.,  95.,  13.,  11.,   3.,   7.,   4.,  16.,  27., 792.]])\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
